{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import common done\n",
      "1.알파뱃 찾기\n",
      "IN : ['aaabbbcccddsgjs', '', 'adjkfeklsjdfk', '', 'jvsklfjsdklf', '', 'jsdfadffsdf', '']\n",
      "OUT : ['aaabbbcccddsgjs', '', 'adjkfeklsjdfk', '', 'jvsklfjsdklf', '', 'jsdfadffsdf', '']\n",
      "\n",
      "\n",
      "2.알파뱃 두단어 찾기\n",
      "IN : ['aaabb', 'bcccd', 'dsgjs', '', 'adjkf', 'eklsj', 'dfk', '', 'jvskl', 'fjsdk', 'lf', '', 'jsdfa', 'dffsd', 'f', '']\n",
      "OUT : ['aaabb', 'bcccd', 'dsgjs', '', 'adjkf', 'eklsj', 'dfk', '', 'jvskl', 'fjsdk', 'lf', '', 'jsdfa', 'dffsd', 'f', '']\n",
      "\n",
      "\n",
      "3.전화번호 패턴 추출\n",
      "IN : 010-9999-9999, 019-2222-4444, 082-1111-3333, 112, 02-111-1111, 3333-3333\n",
      "OUT : ['010-9999-9999', '019-2222-4444', '082-1111-3333', '02-111-1111']\n",
      "\n",
      "\n",
      "4.전화번호 패턴 추출 (or 문 사용)\n",
      "IN : 010-9999-9999, 019-2222-4444, 082-1111-3333, 112, 02-111-1111, 3333-3333\n",
      "OUT : ['010-9999-9999', '019-2222-4444', '082-1111-3333', '02-111-1111', '3333-3333']\n",
      "\n",
      "\n",
      "5.특정 패턴 제외하고 찾기\n",
      "IN : aaabbbcccddsgjs adjkfeklsjdfk jvsklfjsdklf jsdfadffsdf\n",
      "OUT : [' jsdfadffsdf']\n",
      "\n",
      "\n",
      "5.한글만 다 찾아 보기\n",
      "IN : <html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\n",
      "OUT : ['가나다라', '마바사아']\n",
      "\n",
      "\n",
      "6.Title Tag만 다 찾아보기\n",
      "IN : <html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\n",
      "OUT : ['abcd']\n",
      "\n",
      "\n",
      "7.P Tag만 다 찾아보기\n",
      "IN : <html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\n",
      "OUT : ['가나다라', '마바사아']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re, json, os, random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functools import reduce \n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "url = \"{0}:{1}\".format(os.environ['HOSTNAME'] , \"8000\")\n",
    "nn_id = \"nn123\"\n",
    "nn_wf_ver_id =\"1\"\n",
    "\n",
    "print(\"import common done\")\n",
    "import re \n",
    "\n",
    "data = \"aaabbbcccddsgjs adjkfeklsjdfk jvsklfjsdklf jsdfadffsdf\"\n",
    "tel_no_list = \"010-9999-9999, 019-2222-4444, 082-1111-3333, 112, 02-111-1111, 3333-3333\"\n",
    "virtual_html = \"<html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\"\n",
    "\n",
    "#알파뱃(a~z) 중에서 단어를 찾는다 \n",
    "reg = re.compile('[a-z]*')\n",
    "print(\"1.알파뱃 찾기\")\n",
    "print(\"IN : {0}\".format(reg.findall(data)))\n",
    "print(\"OUT : {0}\".format(reg.findall(data)))\n",
    "print('\\n')\n",
    "\n",
    "#알파뱃(a~z) 중에서 두단어를 찾는다 \n",
    "reg = re.compile('[a-z]{0,5}')\n",
    "print(\"2.알파뱃 두단어 찾기\")\n",
    "print(\"IN : {0}\".format(reg.findall(data)))\n",
    "print(\"OUT : {0}\".format(reg.findall(data)))\n",
    "print('\\n')\n",
    "\n",
    "# 특정 패턴에 해당하는 전화번호를 다 찾아보자 \n",
    "reg = re.compile('\\d{2,3}-\\d{3,4}-\\d{4,4}')\n",
    "print(\"3.전화번호 패턴 추출\")\n",
    "print(\"IN : {0}\".format(tel_no_list))\n",
    "print(\"OUT : {0}\".format(reg.findall(tel_no_list)))\n",
    "print('\\n')\n",
    "\n",
    "# 특정 패턴에 해당하는 전화번호를 다 찾아보자 \n",
    "# 3333-3333 도 찾아보자 \n",
    "reg = re.compile('(\\d{2,3}-\\d{3,4}-\\d{4,4}|\\d{3,4}-\\d{4,4})')\n",
    "out = reg.findall(tel_no_list)\n",
    "print(\"4.전화번호 패턴 추출 (or 문 사용)\")\n",
    "print(\"IN : {0}\".format(tel_no_list))\n",
    "print(\"OUT : {0}\".format(reg.findall(tel_no_list)))\n",
    "print('\\n')\n",
    "\n",
    "# 특정 문자를 제외하고 \n",
    "reg = re.compile('[^a]{1,1}[\\w]+$')\n",
    "print(\"5.특정 패턴 제외하고 찾기\")\n",
    "print(\"IN : {0}\".format(data))\n",
    "print(\"OUT : {0}\".format(reg.findall(data)))\n",
    "print('\\n')\n",
    "\n",
    "# 한글 전체 찾기\n",
    "reg = re.compile('[가-힣]{1,}')\n",
    "print(\"5.한글만 다 찾아 보기\")\n",
    "print(\"IN : {0}\".format(virtual_html))\n",
    "print(\"OUT : {0}\".format(reg.findall(virtual_html)))\n",
    "print('\\n')\n",
    "\n",
    "# title 태그 안에 있는 것만 다 가지고 와보자\n",
    "# (xxx) 은 추출이다 \n",
    "reg = re.compile('<title[^>]*>([^<]+)</title>')\n",
    "print(\"6.Title Tag만 다 찾아보기\")\n",
    "print(\"IN : {0}\".format(virtual_html))\n",
    "print(\"OUT : {0}\".format(reg.findall(virtual_html)))\n",
    "print('\\n')\n",
    "\n",
    "# p 태그 안에 있는 것만 다 가지고 와보자\n",
    "reg = re.compile('<p>([\\w]+)</p>')\n",
    "print(\"7.P Tag만 다 찾아보기\")\n",
    "print(\"IN : {0}\".format(virtual_html))\n",
    "print(\"OUT : {0}\".format(reg.findall(virtual_html)))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>NAVER</title>\n"
     ]
    }
   ],
   "source": [
    "def crawler(iter) : \n",
    "    url = \"http://naver.com\"\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text , 'lxml')\n",
    "    for raw in soup.find_all('title') : \n",
    "        print(raw)\n",
    "crawler(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Job Start!!\n",
      "href : https://ko.wikipedia.org/wiki/\n",
      "href : https://ko.wiktionary.org/wiki/\n",
      "href : https://ko.wiktionary.org/wiki/\n",
      "href : https://ko.wikinews.org/wiki/\n",
      "href : https://ko.wikinews.org/wiki/\n",
      "href : https://ko.wikisource.org/wiki/\n",
      "href : https://ko.wikisource.org/wiki/\n",
      "href : https://ko.wikiversity.org/wiki/\n",
      "href : https://ko.wikiversity.org/wiki/\n",
      "href : https://ko.wikivoyage.org/wiki/\n",
      "href : https://ko.wikivoyage.org/wiki/%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikiquote.org/wiki/\n",
      "href : https://ko.wikiquote.org/wiki/\n",
      "href : https://ko.wikibooks.org/wiki/\n",
      "href : https://ko.wikibooks.org/wiki/\n",
      "href : https://ko.wikipedia.org/w/index.php?title=위키백과:대문&oldid=15252069\n",
      "href : https://ko.wikibooks.org/wiki/%EC%9C%84%ED%82%A4%EC%B1%85:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikinews.org/wiki/%EC%9C%84%ED%82%A4%EB%89%B4%EC%8A%A4:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikiquote.org/wiki/%EC%9C%84%ED%82%A4%EC%9D%B8%EC%9A%A9%EC%A7%91:%EB%93%A4%EB%A8%B8%EB%A6%AC\n",
      "href : https://ko.wikisource.org/wiki/%EC%9C%84%ED%82%A4%EB%AC%B8%ED%97%8C:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikiversity.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B0%EC%9B%80%ED%84%B0:%EB%8C%80%EB%AC%B8\n",
      "# Job Done!!\n"
     ]
    }
   ],
   "source": [
    "def task(page, max_pages, url_path, file_w, reg = None):\n",
    "    \"\"\"\n",
    "    지정된 수만큼 제귀 형태로 모든 링크를 따라가서 전부 수집한다. \n",
    "    \"\"\"\n",
    "    if page == max_pages :\n",
    "        get_single_article(url_path, file_w, reg_exp=str(reg))\n",
    "        table_to_csv(url_path)\n",
    "    else : \n",
    "        get_single_article(url_path, file_w, reg_exp=str(reg))\n",
    "        table_to_csv(url_path)\n",
    "        source_code = requests.get(url_path)\n",
    "        plain_text = source_code.text\n",
    "        soup = BeautifulSoup(plain_text, 'lxml')\n",
    "        page += 1\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if (href != None and re.search(\"https://ko\", href)) : \n",
    "                task(page, max_pages, href, file_w, reg=str(reg))\n",
    "\n",
    "def get_single_article(item_url, file_w, reg_exp = None):\n",
    "    \"\"\"\n",
    "    p 태그를 가지고와서 파싱하거나 \n",
    "    지정된 reg_exp 를 사용하여 파싱한다 \n",
    "    \"\"\"\n",
    "    print(\"href : {0}\".format(item_url))\n",
    "    source_code = requests.get(item_url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    \n",
    "    if(reg_exp) : \n",
    "        #정규 표현식이 있는 경우 해당 정규 표현식에 맞는 데이터를 추출 \n",
    "        reg = re.compile(reg_exp)\n",
    "        for contents in reg.findall(plain_text):\n",
    "            file_w.write(contents)\n",
    "    else : \n",
    "        #별도의 Regex가 없는 경우 p tag 에 있는 모든 데이터 추출 \n",
    "        for contents in soup.find_all('p'):\n",
    "            file_w.write(contents.text)\n",
    "\n",
    "def spider(max_pages, url_path, path = \"/home/dev/wiki/\", file_name='test.txt', reg_exp = None) :\n",
    "    \"\"\"\n",
    "    본 Function 을 실행하면 WikiPedia 첫 페이지에서 실행해서 \n",
    "    지정된 횟수만큼 페이지를 따라 들어가서 정해진 패턴을 수집한다. \n",
    "    max_pages : 몇번 Page를 따라 들어갈 것인가를 정의하는 변수 \n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(''.join([path, file_name]), \"w\") as file_w :   \n",
    "        print(\"# Job Start!!\")\n",
    "        task(1, max_pages, url_path, file_w, reg = reg_exp)\n",
    "        print(\"# Job Done!!\")\n",
    "\n",
    "def save_as_csv(data) :\n",
    "    \"\"\"\n",
    "    랜덤하게 csv 이름을 생성하여 주어진 데이터를 저장 \n",
    "    \"\"\"\n",
    "    rand_name = random.randrange(1,10000)\n",
    "    save_data = StringIO(data)\n",
    "    df = pd.read_csv(save_data, sep=\",\")\n",
    "    df.to_csv(\"/home/dev/csv/\" + str(rand_name) + \".csv\", sep=',', encoding='utf-8')\n",
    "    print(\"file saved as : {0}\".format(str(rand_name)))\n",
    "    \n",
    "def table_to_csv(url) : \n",
    "    \"\"\"\n",
    "    전형적인 형태의 Table 을 Parsing 하여 csv 로 저장한다 \n",
    "    \"\"\"\n",
    "    try : \n",
    "        return_line = []\n",
    "        return_td = \"\"\n",
    "        source_code = requests.get(url)\n",
    "        plain_text = source_code.text\n",
    "        soup = BeautifulSoup(plain_text, 'lxml')\n",
    "        for table in soup.find_all('table') :\n",
    "            for tr in table.find_all('tr') :\n",
    "                for td in table.find_all('td') :\n",
    "                    return_td = return_td + td.text + ','\n",
    "                return_td = return_td.rstrip(',') + '\\\\n'\n",
    "            # save each table \n",
    "            save_as_csv(return_td)\n",
    "            return_td = \"\"\n",
    "    except Exception as e : \n",
    "        return True\n",
    "        \n",
    "# 주어진 횟수만큼 해당 사이트를 시작으로 크롤링 시작 \n",
    "# first parm : Inception 횟수 \n",
    "# second parm : initial site \n",
    "# reg_exp : 정규 표현식 사용 가능 \n",
    "\n",
    "# (1) 정규 표현식 사용 CASE (한글 전체 추출)\n",
    "spider(2, 'https://ko.wikipedia.org/wiki/', reg_exp ='[가-힣\\s]{1,}')\n",
    "#spider(2, 'https://ko.wikipedia.org/wiki/', reg_exp ='<title[^>]*>([^<]+)</title>')\n",
    "\n",
    "# (2) 정규 표현식 사용하지 않고 P 태크 추출\n",
    "#spider(2, 'https://ko.wikipedia.org/wiki/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
