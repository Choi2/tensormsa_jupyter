{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Vector & Dict Data Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kor_model.data_crawler import mecab\n",
    "from kor_model.data_embed_model import build_data\n",
    "from kor_model.config import config\n",
    "from kor_model.ner_model.lstmcrf_model import NERModel\n",
    "from kor_model.general_utils import get_logger\n",
    "from kor_model.data_embed_model import data_utils\n",
    "from kor_model.data_embed_model.data_utils import CoNLLDataset\n",
    "from kor_model.data_embed_model import word2vec\n",
    "from kor_model.data_embed_model import data_utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (1) Train 파일을 Mecab 으로 Sentence Spliting & Morphing 작업 수행\n",
    "#mecab.tockenizer(config.train_filename, config.pos_path)\n",
    "\n",
    "# (2) Word2Vec 를 이용하여 단어 단위로 Embedding Vector 를 구성 \n",
    "embed_model = word2vec.train_w2v(config)\n",
    "\n",
    "\n",
    "# (3) Generators Class 생성 Iterator \n",
    "dev   = CoNLLDataset(config.dev_filename, max_iter=config.max_iter)\n",
    "test  = CoNLLDataset(config.test_filename, max_iter=config.max_iter)\n",
    "train = CoNLLDataset(config.train_filename, max_iter=config.max_iter)\n",
    "\n",
    "# (4) Data Set 에서 Word 와 Tag Distinct Value 를 추출 \n",
    "vocab_words, vocab_tags = data_utils.get_vocabs([train, dev, test])\n",
    "\n",
    "# (5) Word Embedding 에 등록된 Dict 와 훈련 Data Set 에 공통으로 있는 것만 사용 \n",
    "vocab = vocab_words & set(embed_model.wv.index2word)\n",
    "vocab.add(data_utils.UNK)\n",
    "\n",
    "# (6) 훈련 데이터에서 Char Dict 추출 \n",
    "vocab_chars = data_utils.get_char_vocab(train)\n",
    "\n",
    "# (7) 모든 Dict 리스트 및 Vector 파일을 저장함 \n",
    "# Char, Word, Tag 3가지에 대하여 Vector 변환을 위한 데이터 \n",
    "data_utils.write_char_embedding(vocab_chars, config.charembed_filename)\n",
    "data_utils.write_vocab(vocab_chars, config.chars_filename)\n",
    "data_utils.write_vocab(vocab, config.words_filename)\n",
    "data_utils.write_vocab(vocab_tags, config.tags_filename)\n",
    "data_utils.export_trimmed_glove_vectors(vocab, embed_model, config.trimmed_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Object Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (8) 위에서 저장한 파일들을 로드\n",
    "embeddings = data_utils.get_trimmed_glove_vectors(config.trimmed_filename)\n",
    "char_embedding = data_utils.get_trimmed_glove_vectors(config.charembed_filename)\n",
    "vocab_words = data_utils.load_vocab(config.words_filename)\n",
    "vocab_tags = data_utils.load_vocab(config.tags_filename)\n",
    "vocab_chars = data_utils.load_vocab(config.chars_filename)\n",
    "\n",
    "# (9) 데이터 필터링 작업을 위한 Method \n",
    "processing_word = data_utils.get_processing_word(vocab_words,\n",
    "                                                 vocab_chars,\n",
    "                                                 lowercase=config.lowercase,\n",
    "                                                 chars=config.chars)\n",
    "processing_tag = data_utils.get_processing_word(vocab_tags,\n",
    "                                                lowercase=False)\n",
    "\n",
    "# 최종적으로 훈련에 사용하는 데이터 객체 (Iterator)\n",
    "dev = CoNLLDataset(config.dev_filename, processing_word, processing_tag, config.max_iter)\n",
    "test = CoNLLDataset(config.test_filename, processing_word, processing_tag, config.max_iter)\n",
    "train = CoNLLDataset(config.train_filename, processing_word, processing_tag, config.max_iter)\n",
    "\n",
    "# build model\n",
    "model = NERModel(config, embeddings, ntags=len(vocab_tags),nchars=len(vocab_chars), logger=None, char_embed=char_embedding)\n",
    "model.build()\n",
    "model.train(train, dev, vocab_tags)\n",
    "model.evaluate(test, vocab_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from results/crf4/model.weights/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from results/crf4/model.weights/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '날짜', '호텔', '예약', '줄레']\n",
      "['B-DATE', 'O', 'B-PURP', 'O', 'O']\n",
      "INFO:tensorflow:Restoring parameters from results/crf4/model.weights/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from results/crf4/model.weights/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['판교', '오늘', '피자', '주문', '줄레']\n",
      "['B-LOC', 'B-DATE', 'B-PURP', 'O', 'O']\n",
      "INFO:tensorflow:Restoring parameters from results/crf4/model.weights/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from results/crf4/model.weights/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '날짜', '판교', '여행', '정보', '줄레']\n",
      "['B-DATE', 'O', 'B-LOC', 'B-PURP', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "model.predict(vocab_tags, processing_word, \"오늘 날짜 호텔 예약 줄레\")\n",
    "model.predict(vocab_tags, processing_word, \"판교 오늘 피자 주문 줄레\")\n",
    "model.predict(vocab_tags, processing_word, \"오늘 날짜 판교 여행 정보 줄레\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
