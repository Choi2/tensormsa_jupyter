{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebCrawler Test\n",
    "WebCrawler는 기본적으로 http 통신을 통해서 얻을 수 있는 HTML 데이터에서 우리가 원하는 데이터를 뽑아내는 과정이라고 볼 수 있다. <br>\n",
    "python에서는 request 라는 통신 라이브러리와 BeauifulSoup라는 Common한 수준에서의 HTML Tag를 추출하는 기능으르 제공하는 라이브러리를 사용하여 간단하게 구현해 볼 수 있다. 기본적으로 추출하고자 하는 데이터가 Json 이나 XML 처럼 정규화된 형태가 없기 때문에 각 사이트 별로 로직을 다르게 구성할 필요가 있을 것이라고 생각된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import common done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re, json, os, random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functools import reduce \n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "url = \"{0}:{1}\".format(os.environ['HOSTNAME'] , \"8000\")\n",
    "nn_id = \"nn123\"\n",
    "nn_wf_ver_id =\"1\"\n",
    "\n",
    "print(\"import common done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규 표현식 테스트\n",
    "정규표현식은 Crawler 에서 필수적인 기능으로 사용법 몇 가지를 테스트해보도록 하겠다. 아래와 같이 몇가지 정규 표현식 예제들을 만들어 보았다. 정규 표현식은 매우 유용하지만 Cralwer 를 개발시에는 조금 불편한 부분들이 있을 수 있다. 이러한 경우 BeautifulSoup 라는 라이브러리를 사용하여 간단하게 사용할 수도 있지만, 정규표현식은 여전히 중요하다고 생각된다. \n",
    "- ^ : 시작 \n",
    "- $ : 끝\n",
    "- [] : 문자 , 예) [a-z]\n",
    "- {최소, 최대} : 예) [a-z]{2,3} \n",
    "- '+' : '{1,}'\n",
    "- '?' : '{0,1}'\n",
    "- '*' : '{0,}'  \n",
    "- '.' : 모든 문자 가능\n",
    "- \\d : 모든 숫자 \n",
    "- \\w : 모든 문자 \n",
    "- [^a] : a를 제외한 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.알파뱃 찾기\n",
      "IN : ['aaabbbcccddsgjs', '', 'adjkfeklsjdfk', '', 'jvsklfjsdklf', '', 'jsdfadffsdf', '']\n",
      "OUT : ['aaabbbcccddsgjs', '', 'adjkfeklsjdfk', '', 'jvsklfjsdklf', '', 'jsdfadffsdf', '']\n",
      "\n",
      "\n",
      "2.알파뱃 두단어 찾기\n",
      "IN : ['aaabb', 'bcccd', 'dsgjs', '', 'adjkf', 'eklsj', 'dfk', '', 'jvskl', 'fjsdk', 'lf', '', 'jsdfa', 'dffsd', 'f', '']\n",
      "OUT : ['aaabb', 'bcccd', 'dsgjs', '', 'adjkf', 'eklsj', 'dfk', '', 'jvskl', 'fjsdk', 'lf', '', 'jsdfa', 'dffsd', 'f', '']\n",
      "\n",
      "\n",
      "3.전화번호 패턴 추출\n",
      "IN : 010-9999-9999, 019-2222-4444, 082-1111-3333, 112, 02-111-1111, 3333-3333\n",
      "OUT : ['010-9999-9999', '019-2222-4444', '082-1111-3333', '02-111-1111']\n",
      "\n",
      "\n",
      "4.전화번호 패턴 추출 (or 문 사용)\n",
      "IN : 010-9999-9999, 019-2222-4444, 082-1111-3333, 112, 02-111-1111, 3333-3333\n",
      "OUT : ['010-9999-9999', '019-2222-4444', '082-1111-3333', '02-111-1111', '3333-3333']\n",
      "\n",
      "\n",
      "5.특정 패턴 제외하고 찾기\n",
      "IN : aaabbbcccddsgjs adjkfeklsjdfk jvsklfjsdklf jsdfadffsdf\n",
      "OUT : [' jsdfadffsdf']\n",
      "\n",
      "\n",
      "5.한글만 다 찾아 보기\n",
      "IN : <html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\n",
      "OUT : ['가나다라', '마바사아']\n",
      "\n",
      "\n",
      "6.Title Tag만 다 찾아보기\n",
      "IN : <html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\n",
      "OUT : ['abcd']\n",
      "\n",
      "\n",
      "7.P Tag만 다 찾아보기\n",
      "IN : <html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\n",
      "OUT : ['가나다라', '마바사아']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "data = \"aaabbbcccddsgjs adjkfeklsjdfk jvsklfjsdklf jsdfadffsdf\"\n",
    "tel_no_list = \"010-9999-9999, 019-2222-4444, 082-1111-3333, 112, 02-111-1111, 3333-3333\"\n",
    "virtual_html = \"<html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\"\n",
    "\n",
    "#알파뱃(a~z) 중에서 단어를 찾는다 \n",
    "reg = re.compile('[a-z]*')\n",
    "print(\"1.알파뱃 찾기\")\n",
    "print(\"IN : {0}\".format(reg.findall(data)))\n",
    "print(\"OUT : {0}\".format(reg.findall(data)))\n",
    "print('\\n')\n",
    "\n",
    "#알파뱃(a~z) 중에서 두단어를 찾는다 \n",
    "reg = re.compile('[a-z]{0,5}')\n",
    "print(\"2.알파뱃 두단어 찾기\")\n",
    "print(\"IN : {0}\".format(reg.findall(data)))\n",
    "print(\"OUT : {0}\".format(reg.findall(data)))\n",
    "print('\\n')\n",
    "\n",
    "# 특정 패턴에 해당하는 전화번호를 다 찾아보자 \n",
    "reg = re.compile('\\d{2,3}-\\d{3,4}-\\d{4,4}')\n",
    "print(\"3.전화번호 패턴 추출\")\n",
    "print(\"IN : {0}\".format(tel_no_list))\n",
    "print(\"OUT : {0}\".format(reg.findall(tel_no_list)))\n",
    "print('\\n')\n",
    "\n",
    "# 특정 패턴에 해당하는 전화번호를 다 찾아보자 \n",
    "# 3333-3333 도 찾아보자 \n",
    "reg = re.compile('(\\d{2,3}-\\d{3,4}-\\d{4,4}|\\d{3,4}-\\d{4,4})')\n",
    "out = reg.findall(tel_no_list)\n",
    "print(\"4.전화번호 패턴 추출 (or 문 사용)\")\n",
    "print(\"IN : {0}\".format(tel_no_list))\n",
    "print(\"OUT : {0}\".format(reg.findall(tel_no_list)))\n",
    "print('\\n')\n",
    "\n",
    "# 특정 문자를 제외하고 \n",
    "reg = re.compile('[^a]{1,1}[\\w]+$')\n",
    "print(\"5.특정 패턴 제외하고 찾기\")\n",
    "print(\"IN : {0}\".format(data))\n",
    "print(\"OUT : {0}\".format(reg.findall(data)))\n",
    "print('\\n')\n",
    "\n",
    "# 한글 전체 찾기\n",
    "reg = re.compile('[가-힣]{1,}')\n",
    "print(\"5.한글만 다 찾아 보기\")\n",
    "print(\"IN : {0}\".format(virtual_html))\n",
    "print(\"OUT : {0}\".format(reg.findall(virtual_html)))\n",
    "print('\\n')\n",
    "\n",
    "# title 태그 안에 있는 것만 다 가지고 와보자\n",
    "# (xxx) 은 추출이다 \n",
    "reg = re.compile('<title[^>]*>([^<]+)</title>')\n",
    "print(\"6.Title Tag만 다 찾아보기\")\n",
    "print(\"IN : {0}\".format(virtual_html))\n",
    "print(\"OUT : {0}\".format(reg.findall(virtual_html)))\n",
    "print('\\n')\n",
    "\n",
    "# p 태그 안에 있는 것만 다 가지고 와보자\n",
    "reg = re.compile('<p>([\\w]+)</p>')\n",
    "print(\"7.P Tag만 다 찾아보기\")\n",
    "print(\"IN : {0}\".format(virtual_html))\n",
    "print(\"OUT : {0}\".format(reg.findall(virtual_html)))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup 간단하게 연결해 보기\n",
    "아래는 간단하게 Naver.com 에 접근하여 Title Tag 값을 추출하는 예제이다. 보는 것처럼 requests 를 호출해서 HTML 데이터를 얻은 후 이 데이터를\n",
    "BeautifulSoup 를 사용해서 필요한 Tag 를 손쉽게 추출 할 수 있다. 정규식아라고 하면 아래와 같은 형태로 구현이 되어야 할 것이지만 라이브러리를 사용하면 더욱 손 쉽게 사용가능하다. <br>\n",
    "- reg = re.compile(\"<title[^>]*>([^<]+)</title>\") <br>\n",
    "- soup.find_all('title')<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>NAVER</title>\n"
     ]
    }
   ],
   "source": [
    "def crawler(iter) : \n",
    "    url = \"http://naver.com\"\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text , 'lxml')\n",
    "    for raw in soup.find_all('title') : \n",
    "        print(raw)\n",
    "crawler(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup 간단하게 Table Parsing 해보기\n",
    "HTML Table 을 Parsing 해서 간단하게 csv 로 바꿔보자. BeautifulSoup 이 얼마나 편한지 알 수 있다. 마치 Jquery 를 하는것 처럼 Elements 를 따라가면서 Find method 를 사용하고 마치 JQuery Iter 작업 처럼 복수의 요소를 쉽게 검색할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv save done\n"
     ]
    }
   ],
   "source": [
    "# 엄청간단하게 table 을 파싱할 수 있다 \n",
    "def crawler_csv() : \n",
    "    url = \"https://race.kra.co.kr/raceScore/ObjtRaceRaceList.do?Act=04&Sub=3&meet=3\"\n",
    "    return_line = []\n",
    "    return_header = \"\"\n",
    "    return_td = \"\"\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    \n",
    "    div = soup.find('div', class_=\"tableType2\")\n",
    "    for th in div.find_all('th') : \n",
    "        return_header = return_header + th.text + ','\n",
    "    for tr in div.find_all('tr') :\n",
    "        for td in tr.find_all('td') : \n",
    "            return_td = return_td + td.text + ','\n",
    "        return_td = return_td.rstrip(',') + '\\\\n'\n",
    "    return (return_header.rstrip(',') + '\\\\n' + return_td)\n",
    "    \n",
    "test_set = crawler_csv()\n",
    "# pandas 를 이용해서 컨버팅한 데이터가 정상인지 한번 확인해 보자\n",
    "test_data = StringIO(test_set)\n",
    "#print(test_data)\n",
    "df = pd.read_csv(test_data, sep=\",\")\n",
    "df.to_csv(\"/home/dev/csv/test.csv\", sep=',', encoding='utf-8')\n",
    "print(\"csv save done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Site 에 존재하는 복수의 테이블을 저장해 보자\n",
    "물론 여기서는 Table , Tr, Td 의 전형적인 구조라고 가정한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_as_csv(data) :\n",
    "    \"\"\"\n",
    "    랜덤하게 csv 이름을 생성하여 주어진 데이터를 저장 \n",
    "    \"\"\"\n",
    "    rand_name = random.randrange(1,10000)\n",
    "    save_data = StringIO(data)\n",
    "    df = pd.read_csv(save_data, sep=\",\")\n",
    "    df.to_csv(\"/home/dev/csv/\" + str(rand_name) + \".csv\", sep=',', encoding='utf-8')\n",
    "    \n",
    "def table_to_csv(url) : \n",
    "    \"\"\"\n",
    "    전형적인 형태의 Table 을 Parsing 하여 csv 로 저장한다 \n",
    "    \"\"\"\n",
    "    return_line = []\n",
    "    return_td = \"\"\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    for table in soup.find_all('table') :\n",
    "        for tr in table.find_all('tr') :\n",
    "            for td in table.find_all('td') :\n",
    "                return_td = return_td + td.text + ','\n",
    "            return_td = return_td.rstrip(',') + '\\\\n'\n",
    "        # save each table \n",
    "        save_as_csv(return_td)\n",
    "        return_td = \"\"\n",
    "    \n",
    "table_to_csv(\"https://ko.wikipedia.org/wiki/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia 한글 페이지 Link를 따라가면서 데이터 수집\n",
    "아주 간단한 WebCralwer 를 테스트 해보았다. 간단하게 WikiPedia 한글 사이트를 처음으로 시작해서 해당 사이트에 존재하는 \"P\" 태그를 수집하고 해당 페이지에서 존재하는 Link를 찾아서 이동하고, 해당 페이지에서 \"P\" 태그를 찾아서 저장하는 행위를 반복하는 코드이다.  <br>\n",
    "별도의 정규 표현식을 이용하고 싶을 경우에는 정규 표현식을 파라메터로 받아서 해당 정규 표현식으로 Crawler 작업을 실행한다. <br> \n",
    "메서드는 spider(2, 'https://ko.wikipedia.org/wiki/') 형태로 되어 있으며, 첫 번째 파레메터는 Inception Level로 몇번이나, Link를 따라 들어가서 Crawler 작업을 수행할 것인지를 지정하는 작업이고, 두 번째 파라메터는 시작할 사이트의 주소가 되겠다. 실제로 실행시 지정한 페이지뿐만 아니라 연결된 Link들을 계속 찾아서 필요한 데이터를 추출하는 것을 볼 수 있다. 세번째 파라메터로 정규 표현식을 입력 받으며, 해당 값이 있는 경우 정규 표현식을 활용하여 파싱 작업을 수행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Job Start!!\n",
      "href : https://ko.wikipedia.org/wiki/\n",
      "file saved as : 4778\n",
      "file saved as : 1311\n",
      "file saved as : 6489\n",
      "file saved as : 1375\n",
      "href : https://ko.wiktionary.org/wiki/\n",
      "href : https://ko.wiktionary.org/wiki/\n",
      "href : https://ko.wikinews.org/wiki/\n",
      "href : https://ko.wikinews.org/wiki/\n",
      "href : https://ko.wikisource.org/wiki/\n",
      "href : https://ko.wikisource.org/wiki/\n",
      "href : https://ko.wikiversity.org/wiki/\n",
      "file saved as : 8018\n",
      "file saved as : 4965\n",
      "file saved as : 1435\n",
      "href : https://ko.wikiversity.org/wiki/\n",
      "file saved as : 3027\n",
      "file saved as : 8234\n",
      "file saved as : 8343\n",
      "href : https://ko.wikivoyage.org/wiki/\n",
      "file saved as : 6115\n",
      "href : https://ko.wikivoyage.org/wiki/%EB%8C%80%EB%AC%B8\n",
      "file saved as : 2508\n",
      "href : https://ko.wikiquote.org/wiki/\n",
      "file saved as : 5292\n",
      "file saved as : 6980\n",
      "file saved as : 9725\n",
      "file saved as : 1458\n",
      "file saved as : 2851\n",
      "href : https://ko.wikiquote.org/wiki/\n",
      "file saved as : 1571\n",
      "file saved as : 4879\n",
      "file saved as : 6796\n",
      "file saved as : 2617\n",
      "file saved as : 8588\n",
      "href : https://ko.wikibooks.org/wiki/\n",
      "href : https://ko.wikibooks.org/wiki/\n",
      "href : https://ko.wikipedia.org/w/index.php?title=위키백과:대문&oldid=15252069\n",
      "file saved as : 7528\n",
      "file saved as : 1696\n",
      "file saved as : 5258\n",
      "file saved as : 1368\n",
      "href : https://ko.wikibooks.org/wiki/%EC%9C%84%ED%82%A4%EC%B1%85:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikinews.org/wiki/%EC%9C%84%ED%82%A4%EB%89%B4%EC%8A%A4:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikiquote.org/wiki/%EC%9C%84%ED%82%A4%EC%9D%B8%EC%9A%A9%EC%A7%91:%EB%93%A4%EB%A8%B8%EB%A6%AC\n",
      "file saved as : 6560\n",
      "file saved as : 8938\n",
      "file saved as : 2502\n",
      "file saved as : 5232\n",
      "file saved as : 1227\n",
      "href : https://ko.wikisource.org/wiki/%EC%9C%84%ED%82%A4%EB%AC%B8%ED%97%8C:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikiversity.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B0%EC%9B%80%ED%84%B0:%EB%8C%80%EB%AC%B8\n",
      "file saved as : 504\n",
      "file saved as : 7003\n",
      "file saved as : 4598\n",
      "# Job Done!!\n"
     ]
    }
   ],
   "source": [
    "def task(page, max_pages, url_path, file_w, reg = None):\n",
    "    \"\"\"\n",
    "    지정된 수만큼 제귀 형태로 모든 링크를 따라가서 전부 수집한다. \n",
    "    \"\"\"\n",
    "    if page == max_pages :\n",
    "        get_single_article(url_path, file_w, reg_exp=str(reg))\n",
    "        table_to_csv(url_path)\n",
    "    else : \n",
    "        get_single_article(url_path, file_w, reg_exp=str(reg))\n",
    "        table_to_csv(url_path)\n",
    "        source_code = requests.get(url_path)\n",
    "        plain_text = source_code.text\n",
    "        soup = BeautifulSoup(plain_text, 'lxml')\n",
    "        page += 1\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if (href != None and re.search(\"https://ko\", href)) : \n",
    "                task(page, max_pages, href, file_w, reg=str(reg))\n",
    "\n",
    "def get_single_article(item_url, file_w, reg_exp = None):\n",
    "    \"\"\"\n",
    "    p 태그를 가지고와서 파싱하거나 \n",
    "    지정된 reg_exp 를 사용하여 파싱한다 \n",
    "    \"\"\"\n",
    "    print(\"href : {0}\".format(item_url))\n",
    "    source_code = requests.get(item_url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    \n",
    "    if(reg_exp) : \n",
    "        #정규 표현식이 있는 경우 해당 정규 표현식에 맞는 데이터를 추출 \n",
    "        reg = re.compile(reg_exp)\n",
    "        for contents in reg.findall(plain_text):\n",
    "            file_w.write(contents)\n",
    "    else : \n",
    "        #별도의 Regex가 없는 경우 p tag 에 있는 모든 데이터 추출 \n",
    "        for contents in soup.find_all('p'):\n",
    "            file_w.write(contents.text)\n",
    "\n",
    "def spider(max_pages, url_path, path = \"/home/dev/wiki/\", file_name='test.txt', reg_exp = None) :\n",
    "    \"\"\"\n",
    "    본 Function 을 실행하면 WikiPedia 첫 페이지에서 실행해서 \n",
    "    지정된 횟수만큼 페이지를 따라 들어가서 정해진 패턴을 수집한다. \n",
    "    max_pages : 몇번 Page를 따라 들어갈 것인가를 정의하는 변수 \n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(''.join([path, file_name]), \"w\") as file_w :   \n",
    "        print(\"# Job Start!!\")\n",
    "        task(1, max_pages, url_path, file_w, reg = reg_exp)\n",
    "        print(\"# Job Done!!\")\n",
    "\n",
    "def save_as_csv(data) :\n",
    "    \"\"\"\n",
    "    랜덤하게 csv 이름을 생성하여 주어진 데이터를 저장 \n",
    "    \"\"\"\n",
    "    rand_name = random.randrange(1,10000)\n",
    "    save_data = StringIO(data)\n",
    "    df = pd.read_csv(save_data, sep=\",\")\n",
    "    df.to_csv(\"/home/dev/csv/\" + str(rand_name) + \".csv\", sep=',', encoding='utf-8')\n",
    "    print(\"file saved as : {0}\".format(str(rand_name)))\n",
    "    \n",
    "def table_to_csv(url) : \n",
    "    \"\"\"\n",
    "    전형적인 형태의 Table 을 Parsing 하여 csv 로 저장한다 \n",
    "    \"\"\"\n",
    "    try : \n",
    "        return_line = []\n",
    "        return_td = \"\"\n",
    "        source_code = requests.get(url)\n",
    "        plain_text = source_code.text\n",
    "        soup = BeautifulSoup(plain_text, 'lxml')\n",
    "        for table in soup.find_all('table') :\n",
    "            for tr in table.find_all('tr') :\n",
    "                for td in table.find_all('td') :\n",
    "                    return_td = return_td + td.text + ','\n",
    "                return_td = return_td.rstrip(',') + '\\\\n'\n",
    "            # save each table \n",
    "            save_as_csv(return_td)\n",
    "            return_td = \"\"\n",
    "    except Exception as e : \n",
    "        return True\n",
    "        \n",
    "# 주어진 횟수만큼 해당 사이트를 시작으로 크롤링 시작 \n",
    "# first parm : Inception 횟수 \n",
    "# second parm : initial site \n",
    "# reg_exp : 정규 표현식 사용 가능 \n",
    "\n",
    "# (1) 정규 표현식 사용 CASE (한글 전체 추출)\n",
    "spider(2, 'https://ko.wikipedia.org/wiki/', reg_exp ='[가-힣\\s]{1,}')\n",
    "#spider(2, 'https://ko.wikipedia.org/wiki/', reg_exp ='<title[^>]*>([^<]+)</title>')\n",
    "\n",
    "# (2) 정규 표현식 사용하지 않고 P 태크 추출\n",
    "#spider(2, 'https://ko.wikipedia.org/wiki/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word2Vec with Hoyai - Net Define\n",
    "Word2Vec 훈련 및 서비스는 Djnago REST Service 기반으로 구현해 보자. 간단하게 사용하고자 하는 네트워크를 정의한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.evaluation result : nn123\n",
      "2.evaluation result : nn123\n",
      "3.evaluation result : word2vec\n"
     ]
    }
   ],
   "source": [
    "# Seq - 1\n",
    "resp = requests.post('http://' + url + '/api/v1/type/common/target/nninfo/nnid/' + nn_id + '/',\n",
    "                     json={\n",
    "                         \"biz_cate\": \"MES\",\n",
    "                         \"biz_sub_cate\": \"M60\",\n",
    "                         \"nn_title\" : \"test\",\n",
    "                         \"nn_desc\": \"test desc\",\n",
    "                         \"use_flag\" : \"Y\",\n",
    "                         \"dir\": \"purpose?\",\n",
    "                         \"config\": \"N\"\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"1.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 2\n",
    "resp = requests.post('http://' + url + '/api/v1/type/common/target/nninfo/nnid/' + nn_id + '/version/',\n",
    "                 json={\n",
    "                     \"nn_def_list_info_nn_id\": \"\",\n",
    "                     \"nn_wf_ver_info\": \"test version info\",\n",
    "                     \"condition\": \"1\",\n",
    "                     \"active_flag\": \"Y\"\n",
    "                 })\n",
    "data = json.loads(resp.json())\n",
    "print(\"2.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 3\n",
    "resp = requests.post('http://' + url + '/api/v1/type/wf/target/init/mode/simple/'+ nn_id + '/wfver/1/',\n",
    "                     json={\n",
    "                         \"type\": \"word2vec\"\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"3.evaluation result : {0}\".format(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with Hoyai - Train Node Define\n",
    "Train 하고자 하는 데이터, POS 전처리 방법 등을 지정한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.evaluation result : ['1 file upload success']\n",
      "5.evaluation result : {'source_type': 'local', 'preprocess': 'mecab', 'store_path': '/hoya_str_root/nn123/1/data_node', 'source_parse_type': 'raw', 'max_sentence_len': 10, 'source_path': '/hoya_src_root/nn123/1/data_node', 'source_sql': 'all', 'source_server': 'local'}\n",
      "6.evaluation result : mecab\n",
      "7.evaluation result : /hoya_str_root/nn123/1/data_node\n",
      "8.evaluation result : {'source_type': 'local', 'preprocess': 'mecab', 'store_path': '/hoya_str_root/nn123/1/data_node', 'source_parse_type': 'raw', 'max_sentence_len': 10, 'source_path': '/hoya_src_root/nn123/1/data_node', 'source_sql': 'all', 'source_server': 'local'}\n"
     ]
    }
   ],
   "source": [
    "# Seq - 4\n",
    "return_dict = {}\n",
    "return_dict['test'] = open('/home/dev/wiki/test.txt', 'rb')\n",
    "\n",
    "resp = requests.post('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/source/nnid/'+nn_id+'/ver/1/node/data_node/',\n",
    "                     files = return_dict)\n",
    "\n",
    "data = json.loads(resp.json())\n",
    "print(\"4.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 5\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/source/nnid/'+ nn_id + '/ver/1/node/data_node/',\n",
    "                     json={\n",
    "                         \"source_server\": \"local\",\n",
    "                         \"source_sql\": \"all\",\n",
    "                         \"max_sentence_len\" : 10\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"5.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 6\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/pre/nnid/'+ nn_id + '/ver/1/node/data_node/',\n",
    "                     json={\n",
    "                         \"preprocess\":  \"mecab\",\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"6.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 7\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/store/nnid/'+ nn_id + '/ver/1/node/data_node/')\n",
    "data = json.loads(resp.json())\n",
    "print(\"7.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 8\n",
    "resp = requests.get('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/source/nnid/'+ nn_id + '/ver/1/node/data_node/')\n",
    "data = json.loads(resp.json())\n",
    "print(\"8.evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with Hoyai - Word2Vec Net 정의\n",
    "- window_size : c-bow 알고리즘으로 테스트 데이터 생성시 기준 단어로부터 얼만큼 떨어진 데이터까지 가지고 올 것인지 지정 \n",
    "- vector_size : embedding 하려는 Vector의 Size (결과의 사이즈가 될 것) \n",
    "- batch_size : HDF5 에서 한번에 가지고올 데이터의 사이즈 \n",
    "- iter : Hidden Layer 하나짜리 신경망 훈련시 몇번 반복할 것인가 \n",
    "- min_count : Dict 구성시 몇번 이상 발생해야 사전에 추가할 것인가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.evaluation result : {'vector_size': 100, 'batch_size': 100, 'min_count': 1, 'window_size': 5, 'model_path': '/hoya_model_root/nn123/1/netconf_node', 'iter': 5}\n"
     ]
    }
   ],
   "source": [
    "# Seq - 9\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/netconf/detail/w2v/nnid/' + nn_id + '/ver/' + nn_wf_ver_id + '/node/netconf_node/',\n",
    "                     json={\n",
    "                        \"window_size\" : 5,\n",
    "                        \"vector_size\" : 100,\n",
    "                        \"batch_size\" : 100,\n",
    "                        \"iter\" : 5,\n",
    "                        \"min_count\" : 1\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"9.evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with Hoyai - 테스트에 사용할 데이터 \n",
    "Train 에 사용하지 않은 새로운 데이터로 테스트를 실행하기 위한 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.evaluation result : ['1 file upload success']\n",
      "11.evaluation result : {'source_type': 'local', 'preprocess': 'mecab', 'store_path': '/hoya_str_root/nn123/1/test_data_node', 'source_parse_type': 'raw', 'max_sentence_len': 50, 'source_path': '/hoya_src_root/nn123/1/test_data_node', 'source_sql': 'all', 'source_server': 'local'}\n",
      "12.evaluation result : mecab\n",
      "13.evaluation result : /hoya_str_root/nn123/1/test_data_node\n",
      "14.evaluation result : {'type': 'w2v'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Seq - 10\n",
    "return_dict = {}\n",
    "return_dict['test'] = open('/home/dev/wiki/test.txt', 'rb')\n",
    "resp = requests.post('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/source/nnid/'+ nn_id + '/ver/1/node/test_data_node/',\n",
    "                     files = return_dict)\n",
    "data = json.loads(resp.json())\n",
    "print(\"10.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 11\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/source/nnid/'+ nn_id + '/ver/1/node/test_data_node/',\n",
    "                     json={\n",
    "                         \"source_server\": \"local\",\n",
    "                         \"source_sql\": \"all\",\n",
    "                         \"max_sentence_len\" : 50\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"11.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 12\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/pre/nnid/'+ nn_id + '/ver/1/node/test_data_node/',\n",
    "                     json={\n",
    "                         \"preprocess\":  \"mecab\",\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"12.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 13\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/store/nnid/'+ nn_id + '/ver/1/node/test_data_node/')\n",
    "data = json.loads(resp.json())\n",
    "print(\"13.evaluation result : {0}\".format(data))\n",
    "\n",
    "node_name = 'eval_node'\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/eval/nnid/'+nn_id+'/ver/1/node/eval_node/',\n",
    "                    json={\n",
    "                        \"type\": \"w2v\",\n",
    "                    })\n",
    "data = json.loads(resp.json())\n",
    "print(\"14.evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with Hoyai - 모델 훈련 시작 ! \n",
    "지금까지 정의한 기준 정보를 가지고 실제 모델 훈련을 시작한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.evaluation result : [None, None, None, None, 0, {'word': [], 'x': [], 'y': []}]\n"
     ]
    }
   ],
   "source": [
    "# Seq - 14\n",
    "resp = requests.post('http://' + url + '/api/v1/type/runmanager/state/train/nnid/'+nn_id+'/ver/'+nn_wf_ver_id+'/')\n",
    "data = json.loads(resp.json())\n",
    "print(\"14.evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector 확인\n",
    "단어를 차원에 Embeddning 한다는 것이 어떤 의미인지 한번 테스트를 해보도록 하자. 단어를 차원에 Embedding 하게 되면 얻는 장점은 명확하다. 단어간의 유사도를 찾아 낼 수 있다는 것. 두번째는 OneHot Encoder 대비 더 적은 차원수로 단어를 표현할 수 있다는 것이 되겠다. 첫번째의 장점은 우리가 복잡한 NLP 서비스를 만들려고 했을때, AI 가 더 다양한 표현들을 잘 이해할 수 있도록 하는데 굉장히 효과적이다. 다만 유의어 동의어 등에 대한 처리에 대해서는 W2v 만으로는 한계가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : [[0.3671122193336487, 0.36258116364479065, -0.153810515999794, 0.3382582664489746, 0.07524526864290237, -0.9237509369850159, 0.586235761642456, -0.7128365635871887, -0.1618472784757614, -1.7198208570480347, -0.45088905096054077, -1.8178998231887817, 0.255445659160614, 0.6051250100135803, 1.6374379396438599, -0.3707364499568939, 0.6106436252593994, -0.010830960236489773, 0.7528746724128723, -0.09685423225164413, -0.17752382159233093, 0.3827434182167053, 0.1822998821735382, -0.8178929686546326, 0.9471759796142578, 0.8136753439903259, 0.324432909488678, 0.6151374578475952, -0.09854627400636673, -0.22025443613529205, 0.7294281721115112, -1.5659046173095703, 0.0959482342004776, -0.34772005677223206, -0.38805946707725525, 0.30053871870040894, 0.7075024247169495, -0.16250531375408173, -0.1269727349281311, 1.136719822883606, -0.10086284577846527, 0.5233539938926697, 0.20110321044921875, -0.6678069233894348, -0.5378274917602539, 0.9875825047492981, -0.49586984515190125, 0.6263409852981567, -0.3483254909515381, -0.09171627461910248, -0.07739520817995071, -0.43517541885375977, 0.3777124881744385, 0.428139328956604, -1.0971167087554932, -0.4844364821910858, -1.094782829284668, -0.456738144159317, 0.1338798701763153, -1.0440620183944702, -0.9751610159873962, 0.8517056703567505, -1.230686068534851, 0.5657521486282349, -0.7323635220527649, -0.5638496279716492, -0.3834339678287506, -0.33365577459335327, 1.028125524520874, 1.874680995941162, 1.4630261659622192, 0.7806723117828369, -0.24949760735034943, 1.1560593843460083, -0.7130166292190552, -1.4342925548553467, 0.5142744183540344, -0.11589750647544861, -0.09730959683656693, 0.8305281400680542, -0.2920929193496704, 0.5210551023483276, 0.8751779794692993, -0.775452733039856, 0.576682448387146, -0.2689771354198456, 0.21541748940944672, -0.2926563322544098, -0.4424210488796234, -0.2012598067522049, 0.2499481737613678, -0.28089314699172974, -0.011807059869170189, -0.23494407534599304, -0.45307737588882446, -0.020275192335247993, -0.7416952252388, -0.5082272887229919, -0.7034950852394104, -0.6710397601127625], [-1.1166542768478394, -0.9613519310951233, 0.6044901013374329, 0.32308465242385864, 1.2222516536712646, -0.4652264416217804, 1.5488364696502686, -0.46237504482269287, -0.78068608045578, 0.19207188487052917, -2.20235013961792, -0.9822607636451721, 1.6453168392181396, -1.1694883108139038, 0.27410948276519775, -0.011928357183933258, -0.1539381891489029, 2.1110832691192627, 0.4047289490699768, 0.39967459440231323, 0.41670727729797363, 0.1837197095155716, 0.47757500410079956, -0.488831490278244, -0.23415088653564453, -0.21626485884189606, -0.6925264000892639, -1.6878039836883545, -0.6652444005012512, -0.10047607868909836, -0.002188397804275155, 0.2227751761674881, -0.451908141374588, -0.0546228289604187, 1.8238906860351562, -0.26736536622047424, 1.329162836074829, 0.07201502472162247, -0.10258638858795166, 1.6304315328598022, 0.2711184620857239, -0.009179546497762203, -0.742067277431488, -0.6783273816108704, -0.09351567178964615, -1.6624553203582764, 0.44806814193725586, 0.8860379457473755, 0.21310989558696747, -1.1144746541976929, -1.2171339988708496, -0.7341612577438354, 0.9521527886390686, -0.39488574862480164, 0.060666345059871674, -0.8768692016601562, -0.07314430177211761, 1.6158318519592285, 0.15068739652633667, -0.1247042715549469, 0.7220286130905151, -0.4033617079257965, 0.9309840202331543, 0.26993292570114136, -2.1658194065093994, 0.8765025734901428, -0.4027405083179474, 0.46219784021377563, -1.2857935428619385, 1.3579312562942505, -0.18507759273052216, -1.4787013530731201, 0.03765285387635231, -0.24068908393383026, 1.1122418642044067, -0.5698127746582031, 0.10128435492515564, 1.2212574481964111, 0.5686291456222534, -0.2507046163082123, 1.8605235815048218, 0.771484911441803, 0.5758460164070129, 0.047864023596048355, -0.11620136350393295, -1.2037992477416992, -1.1808604001998901, 0.36641183495521545, -0.3399377763271332, 0.18882572650909424, 0.21677392721176147, 0.02646547742187977, 2.6589179039001465, -0.1535598635673523, 0.614229142665863, 0.9349245429039001, 0.6721035242080688, -0.28296157717704773, 0.3477517366409302, -0.19263018667697906]]\n"
     ]
    }
   ],
   "source": [
    "# Run All Workflow\n",
    "resp = requests.post('http://' + url + '/api/v1/type/service/state/predict/type/w2v/nnid/' + nn_id + '/ver/active/',\n",
    "                     json={\n",
    "                         \"type\": \"vector\",\n",
    "                         \"val_1\":[\"포털\", \"한복\"],\n",
    "                         \"val_2\":[]\n",
    "                        }\n",
    "                     )\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연관 단어 검색\n",
    "유사도 테스트를 한번 해보자 첫번째 파라메터는 긍정요인, 두번째 파라메터는 부정요인이다. 인터넷에서 우리가 찾아보면 가장 많아 나오는 예제가 바로 king - Man = Queen과 같은 형태로 표현하는 것을 생각해 볼 수 있겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : [[['으로/JKB', 0.999835729598999], ['고/EC', 0.9998282194137573], ['는/JX', 0.9998234510421753], ['한복/NNG', 0.9998130202293396], [',/SC', 0.9998108148574829]]]\n"
     ]
    }
   ],
   "source": [
    "# Run All Workflow\n",
    "resp = requests.post('http://' + url + '/api/v1/type/service/state/predict/type/w2v/nnid/' + nn_id + '/ver/active/',\n",
    "                     json={\n",
    "                         \"type\": \"sim\",\n",
    "                         \"val_1\":[\"어업\"],\n",
    "                         \"val_2\":[\"\"]\n",
    "                     }\n",
    "                     )\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어간의 유사도 테스트\n",
    "두개의 단어를 넣고 유사도를 테스트해 볼 수도 있다. 당연히 1에 가까울 수록 두 단어 간의 상관성이 높다고 판단할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : [-0.2208110638145097]\n"
     ]
    }
   ],
   "source": [
    "# Run All Workflow\n",
    "resp = requests.post('http://' + url + '/api/v1/type/service/state/predict/type/w2v/nnid/' + nn_id + '/ver/active/',\n",
    "                     json={\n",
    "                         \"type\": \"similarity\",\n",
    "                         \"val_1\":[\"포털\"],\n",
    "                         \"val_2\":[\"윤리\"]\n",
    "                     }\n",
    "                     )\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 을 통해서 시각화\n",
    "각 단어의 vecotr 값을 Plot 을 통해서 화면에 출력하여 보자. 입력한 단어의 Vector를 서버에서 받아서 해당 vector 를 2D로 변경한 후에 plot 을 통해서 화면에 출력해 주었다 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAFkCAYAAACThxm6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npKIMF5Ikqaj/D+DDoN51xGtIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3eea746f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run All Workflow\n",
    "resp = requests.post('http://' + url + '/api/v1/type/service/state/predict/type/w2v/nnid/' + nn_id + '/ver/active/',\n",
    "                     json={\n",
    "                         \"type\": \"vector\",\n",
    "                         \"val_1\":[\"포털\", \"한복\",\"한국\", \"으로\"],\n",
    "                         \"val_2\":[]\n",
    "                        }\n",
    "                     )\n",
    "data = json.loads(resp.json())\n",
    "\n",
    "def avg(x,y) : \n",
    "    \"\"\"\n",
    "    그냥 Numpy 써도 됨 재미로.. \n",
    "    \"\"\"\n",
    "    return (x+y)/2\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for raw in data : \n",
    "    x.append(np.var(raw).tolist())\n",
    "    y.append(reduce(lambda x, y: avg(x,y), raw))\n",
    "\n",
    "# 원래 그냥 Gensim 모델을 사용해서 Vecotor를 뿌려주면 됨 \n",
    "# 여기서는 그냥 테스트를 위해서 두개의 백터를 받아서 다차원을 \n",
    "# x 는 평균, y는 분산을 구해서 출력하였음 \n",
    "myvec = np.array([x,y])\n",
    "plt.plot(myvec[0,],myvec[1,],'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
