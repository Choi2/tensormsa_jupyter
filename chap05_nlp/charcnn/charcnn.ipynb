{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.image import imread, imsave \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models import word2vec\n",
    "print(\"load done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_size = 50\n",
    "encode_length = 6\n",
    "label_size = 10\n",
    "embed_type = \"onehot\" #onehot or w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_vector_model(path):\n",
    "    with open (path, \"r\") as myfile : \n",
    "        str_buf = myfile.readlines()\n",
    "\n",
    "    mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "    pos1 = mecab.pos(''.join(str_buf))\n",
    "    pos2 = ' '.join(list(map(lambda x : '\\n' if x[1] in ['SF'] else x[0], pos1))).split('\\n')\n",
    "    morphs = list(map(lambda x : mecab.morphs(x) , pos2))\n",
    "    model = word2vec.Word2Vec(size=vector_size, window=2, min_count=1)\n",
    "    model.build_vocab(morphs)\n",
    "    model.train(morphs)\n",
    "    return model\n",
    "\n",
    "model = train_vector_model(\"./data/train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv(data_path):\n",
    "    df_csv_read = pd.read_csv(data_path,\n",
    "                              skipinitialspace=True,\n",
    "                              engine=\"python\",\n",
    "                              encoding='utf-8-sig')\n",
    "    return df_csv_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embed(data) : \n",
    "    mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for encode_raw in data['encode'] : \n",
    "        encode_raw = mecab.morphs(encode_raw)\n",
    "        encode_raw = list(map(lambda x : encode_raw[x] if x < len(encode_raw) else '#', range(encode_length)))\n",
    "        if(embed_type == 'onehot') :\n",
    "            bucket = np.zeros(vector_size, dtype=float).copy()\n",
    "            input = np.array(list(map(lambda x : onehot_vectorize(bucket, x) if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "        else : \n",
    "            input = np.array(list(map(lambda x : model[x] if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "     \n",
    "        inputs.append(input.flatten())\n",
    "        \n",
    "    for decode_raw in data['decode'] : \n",
    "        label = np.zeros(label_size, dtype=float)\n",
    "        np.put(label, decode_raw, 1)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "def onehot_vectorize(bucket, x):\n",
    "    np.put(bucket, model.wv.index2word.index(x),1)\n",
    "    return bucket\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed word to vector on predict step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_embed(data) : \n",
    "    mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "    encode_raw = mecab.morphs(data)\n",
    "    encode_raw = list(map(lambda x : encode_raw[x] if x < len(encode_raw) else '#', range(encode_length)))\n",
    "    if(embed_type == 'onehot') :\n",
    "        bucket = np.zeros(vector_size, dtype=float).copy()\n",
    "        input = np.array(list(map(lambda x : onehot_vectorize(bucket, x) if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "    else : \n",
    "        input = np.array(list(map(lambda x : model[x] if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get train and test data for feed on tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_data() :\n",
    "    train_data, train_label = embed(load_csv(\"./data/train.csv\"))\n",
    "    test_data, test_label = embed(load_csv(\"./data/test.csv\"))\n",
    "    return train_label, test_label, train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define cnn graph func\n"
     ]
    }
   ],
   "source": [
    "def create_graph(train=True):\n",
    "    # placeholder is used for feeding data.\n",
    "    x = tf.placeholder(\"float\", shape=[None, encode_length * vector_size], name = 'x') \n",
    "    y_target = tf.placeholder(\"float\", shape=[None, label_size], name = 'y_target') \n",
    "\n",
    "    # reshape input data\n",
    "    x_image = tf.reshape(x, [-1,encode_length,vector_size,1], name=\"x_image\")\n",
    "    \n",
    "    # Build a convolutional layer and maxpooling with random initialization\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal([2, 2, 1, 32], stddev=0.1), name=\"W_conv1\") # W is [row, col, channel, feature]\n",
    "    b_conv1 = tf.Variable(tf.zeros([32]), name=\"b_conv1\")\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1, name=\"h_conv1\")\n",
    "    h_pool1 = tf.nn.max_pool( h_conv1 , ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = \"h_pool1\")\n",
    "    \n",
    "    # Build a fully connected layer\n",
    "    h_pool2_flat = tf.reshape(h_pool1, [-1, int((encode_length/2)*(vector_size/2))*32], name=\"h_pool2_flat\")\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([int((encode_length/2)*(vector_size/2))*32, 256], stddev=0.1), name = 'W_fc1')\n",
    "    b_fc1 = tf.Variable(tf.zeros([256]), name = 'b_fc1')\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name=\"h_fc1\")\n",
    "    \n",
    "    keep_prob = 1.0\n",
    "    if(train) : \n",
    "        # Dropout Layer\n",
    "        keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "        h_fc1 = tf.nn.dropout(h_fc1, keep_prob, name=\"h_fc1_drop\")\n",
    "    \n",
    "    # Build a fully connected layer with softmax \n",
    "    W_fc2 = tf.Variable(tf.truncated_normal([256, label_size], stddev=0.1), name = 'W_fc2')\n",
    "    b_fc2 = tf.Variable(tf.zeros([label_size]), name = 'b_fc2')\n",
    "    y=tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2, name=\"y\")\n",
    "    \n",
    "    # define the Loss function\n",
    "    #cross_entropy = -tf.reduce_sum(y_target*tf.log(y), name = 'cross_entropy')\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_target))\n",
    "    \n",
    "    # define optimization algorithm\n",
    "    #train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_target, 1))\n",
    "    # correct_prediction is list of boolean which is the result of comparing(model prediction , data)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "    # tf.cast() : changes true -> 1 / false -> 0\n",
    "    # tf.reduce_mean() : calculate the mean\n",
    "    \n",
    "    # create summary of parameters\n",
    "    tf.summary.histogram('weights_1', W_conv1)\n",
    "    tf.summary.histogram('y', y)\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(\"/tmp/cnn\")\n",
    "    \n",
    "    return accuracy, x, y_target, keep_prob, train_step, merged, y, cross_entropy, summary_writer, W_conv1\n",
    "    \n",
    "print(\"define cnn graph func\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize weight matrix function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_layer(weight_list) :\n",
    "    for matrix in weight_list[0] :  \n",
    "        print(plt.imshow(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-8674f1ae24ea>:12: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "step 0, training accuracy: 0.118\n",
      "summary for tensorboard written\n",
      "step 10, training accuracy: 0.471\n",
      "summary for tensorboard written\n",
      "step 20, training accuracy: 0.765\n",
      "summary for tensorboard written\n",
      "step 30, training accuracy: 0.765\n",
      "summary for tensorboard written\n",
      "step 40, training accuracy: 0.765\n",
      "summary for tensorboard written\n",
      "step 50, training accuracy: 0.706\n",
      "summary for tensorboard written\n",
      "step 60, training accuracy: 0.765\n",
      "summary for tensorboard written\n",
      "step 70, training accuracy: 0.765\n",
      "summary for tensorboard written\n",
      "step 80, training accuracy: 0.882\n",
      "summary for tensorboard written\n",
      "step 90, training accuracy: 1.000\n",
      "summary for tensorboard written\n",
      "step 100, training accuracy: 1.000\n",
      "summary for tensorboard written\n",
      "step 110, training accuracy: 1.000\n",
      "summary for tensorboard written\n",
      "step 120, training accuracy: 1.000\n",
      "summary for tensorboard written\n",
      "step 130, training accuracy: 1.000\n",
      "summary for tensorboard written\n",
      "step 140, training accuracy: 1.000\n",
      "summary for tensorboard written\n",
      "step 150, training accuracy: 1.000\n",
      "summary for tensorboard written\n",
      "step 160, training accuracy: 1.000\n",
      "summary for tensorboard written\n",
      "step 170, training accuracy: 1.000\n",
      "summary for tensorboard written\n",
      "step 180, training accuracy: 1.000\n",
      "summary for tensorboard written\n",
      "step 190, training accuracy: 1.000\n",
      "summary for tensorboard written\n",
      "test accuracy: 1\n",
      "AxesImage(54,36;334.8x223.2)\n",
      "AxesImage(54,36;334.8x223.2)\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "def run() : \n",
    "    try : \n",
    "        # get Data \n",
    "        labels_train, labels_test, data_filter_train, data_filter_test = get_test_data()\n",
    "        # reset Graph\n",
    "        tf.reset_default_graph()   \n",
    "        # Create Session\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))  \n",
    "        # create graph\n",
    "        accuracy, x, y_target, keep_prob, train_step, merged, y, cross_entropy, summary_writer, W_conv1 = create_graph(train=True)\n",
    "        # set saver\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        # initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        # training the MLP\n",
    "        for i in range(200): \n",
    "            sess.run(train_step, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 0.5})\n",
    "            if i%10 == 0:\n",
    "                train_accuracy = sess.run(accuracy, feed_dict={x:data_filter_train, y_target: labels_train, keep_prob: 1})\n",
    "                print (\"step %d, training accuracy: %.3f\"%(i, train_accuracy))\n",
    "                \n",
    "                # calculate the summary and write.\n",
    "                summary = sess.run(merged, feed_dict={x:data_filter_train, y_target: labels_train, keep_prob: 1})\n",
    "                summary_writer.add_summary(summary , i)\n",
    "                print(\"summary for tensorboard written\")\n",
    "                \n",
    "        # for given x, y_target data set\n",
    "        print  (\"test accuracy: %g\"% sess.run(accuracy, feed_dict={x:data_filter_test, y_target: labels_test, keep_prob: 1}))\n",
    "        \n",
    "        # show weight matrix as image \n",
    "        weight_vectors = sess.run(W_conv1, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 1.0})\n",
    "        show_layer(weight_vectors)\n",
    "        \n",
    "        # Save Model\n",
    "        path = './model/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            print(\"path created\")\n",
    "        saver.save(sess, path)\n",
    "        print(\"model saved\")\n",
    "    except Exception as e : \n",
    "        raise Exception (\"error on training: {0}\".format(e))\n",
    "    finally :\n",
    "        sess.close()\n",
    "\n",
    "# run stuff\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in dict : ['다', '이', '정말', '맛', '긍정', '가', '부정', '별로', '다시', '적', '식당', '지', '아깝', '야', '.', '영화', '은', '음식', '있', '돈', '없', '않', '어', '꼭', '재미있', '겠어', '와야', '거', '을', '는', '였', '맛있', '오', '재미없']\n",
      "INFO:tensorflow:Restoring parameters from ./model/\n",
      "model restored\n",
      "result : [array([[  3.39150688e-06,   9.93813753e-01,   6.11760747e-03,\n",
      "          1.16159163e-05,   3.05861090e-06,   3.36837475e-05,\n",
      "          2.23066650e-06,   7.18798628e-06,   5.37696997e-06,\n",
      "          2.05518018e-06]], dtype=float32)]\n",
      "result : 1\n",
      "INFO:tensorflow:Restoring parameters from ./model/\n",
      "model restored\n",
      "result : [array([[  4.07658517e-05,   7.67132267e-02,   9.22763646e-01,\n",
      "          8.06034659e-05,   5.87379909e-05,   2.00696159e-04,\n",
      "          2.19792601e-05,   4.23768033e-05,   6.28341804e-05,\n",
      "          1.50135611e-05]], dtype=float32)]\n",
      "result : 2\n"
     ]
    }
   ],
   "source": [
    "def predict(test_data) : \n",
    "    try : \n",
    "        # reset Graph\n",
    "        tf.reset_default_graph()   \n",
    "        # Create Session\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))  \n",
    "        # create graph\n",
    "        _, x, _, _, _, _, y, _, _, _ = create_graph(train=False)\n",
    "        \n",
    "        # initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # set saver\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        # Restore Model\n",
    "        path = './model/'\n",
    "        if os.path.exists(path):\n",
    "            saver.restore(sess, path)\n",
    "            print(\"model restored\")\n",
    "\n",
    "        # training the MLP\n",
    "        #print(\"input data : {0}\".format(test_data))\n",
    "        y = sess.run([y], feed_dict={x: np.array([test_data])})\n",
    "        print(\"result : {0}\".format(y))\n",
    "        print(\"result : {0}\".format(np.argmax(y)))\n",
    "        \n",
    "    except Exception as e : \n",
    "        raise Exception (\"error on training: {0}\".format(e))\n",
    "    finally :\n",
    "        sess.close()\n",
    "\n",
    "print(\"words in dict : {0}\".format(model.wv.index2word))\n",
    "# run stuff\n",
    "predict(np.array(inference_embed(\"다시는 오지 않을 거야\")).flatten())\n",
    "predict(np.array(inference_embed(\"돈이 아깝지 않다\")).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
