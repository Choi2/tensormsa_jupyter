{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "print(\"load done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define preprocess func\n"
     ]
    }
   ],
   "source": [
    "def change (x) :\n",
    "    dummy = [0.0, 0.0, 0.0]\n",
    "    idx = ['Clear', 'Rain', 'Clouds']\n",
    "    dummy[idx.index(x)] = 1.0\n",
    "    return dummy\n",
    "\n",
    "def to_matrix(data) :\n",
    "    return_arr = []\n",
    "    idxs = ['humidity', 'pressure', 'grnd_level', 'temp_max', 'temp', 'temp_min', 'temp_kf', 'sea_level']\n",
    "    for idx in idxs : \n",
    "        return_arr.append(float(data.get(idx)))\n",
    "    return return_arr \n",
    "\n",
    "print(\"define preprocess func\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define data get func\n"
     ]
    }
   ],
   "source": [
    "def get_test_data() :\n",
    "    resp = requests.get('http://openweathermap.org/data/2.5/forecast?q=London,us&mode=json&appid=b1b15e88fa797225412429c1c50c122a1')\n",
    "    data = resp.json()\n",
    "\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    # parse json\n",
    "    data_list = data['list']\n",
    "    for raw in data_list :\n",
    "        x_data.append(raw['main'])\n",
    "        y_data.append(raw['weather'][0]['main'])\n",
    "\n",
    "    # divide train & test\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(x_data, y_data, test_size=0.20, random_state=42)\n",
    "    \n",
    "    # preprocess data\n",
    "    labels_train = list(map(lambda x : change(x), labels_train ))\n",
    "    labels_test = list(map(lambda x : change(x), labels_test ))\n",
    "    data_filter_train = list(map(lambda x : to_matrix(x), data_train ))\n",
    "    data_filter_test = list(map(lambda x : to_matrix(x), data_test ))\n",
    "    \n",
    "    # Print Data \n",
    "    print(\"data_test : {0}\".format(data_filter_test))\n",
    "    print(\"data_train : {0}\".format(len(data_filter_train)))\n",
    "    print(\"labels_train : {0}\".format(len(labels_train)))\n",
    "    print(\"data_test : {0}\".format(len(data_filter_test)))\n",
    "    print(\"labels_test : {0}\".format(len(labels_test)))\n",
    "\n",
    "    return np.array(labels_train), np.array(labels_test), np.array(data_filter_train), np.array(data_filter_test)\n",
    "\n",
    "print(\"define data get func\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define cnn graph func\n"
     ]
    }
   ],
   "source": [
    "def create_graph(train=True):\n",
    "\n",
    "    # placeholder is used for feeding data.\n",
    "    x = tf.placeholder(\"float\", shape=[None, 8], name = 'x') # none represents variable length of dimension. 784 is the dimension of MNIST data.\n",
    "    y_target = tf.placeholder(\"float\", shape=[None, 3], name = 'y_target') # shape argument is optional, but this is useful to debug.\n",
    "\n",
    "    # reshape input data\n",
    "    x_image = tf.reshape(x, [-1,2,4,1], name=\"x_image\")\n",
    "    \n",
    "    # Build a convolutional layer and maxpooling with random initialization\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal([2, 2, 1, 32], stddev=0.1), name=\"W_conv1\") # W is [row, col, channel, feature]\n",
    "    b_conv1 = tf.Variable(tf.zeros([32]), name=\"b_conv1\")\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1, name=\"h_conv1\")\n",
    "    h_pool1 = tf.nn.max_pool( h_conv1 , ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = \"h_pool1\")\n",
    "    \n",
    "    # Build a fully connected layer\n",
    "    h_pool2_flat = tf.reshape(h_pool1, [-1, 1*2*32], name=\"h_pool2_flat\")\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([1*2*32, 256], stddev=0.1), name = 'W_fc1')\n",
    "    b_fc1 = tf.Variable(tf.zeros([256]), name = 'b_fc1')\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name=\"h_fc1\")\n",
    "    \n",
    "    keep_prob = 1.0\n",
    "    if(train) : \n",
    "        # Dropout Layer\n",
    "        keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "        h_fc1 = tf.nn.dropout(h_fc1, keep_prob, name=\"h_fc1_drop\")\n",
    "    \n",
    "    # Build a fully connected layer with softmax \n",
    "    W_fc2 = tf.Variable(tf.truncated_normal([256, 3], stddev=0.1), name = 'W_fc2')\n",
    "    b_fc2 = tf.Variable(tf.zeros([3]), name = 'b_fc2')\n",
    "    y=tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2, name=\"y\")\n",
    "    \n",
    "    # define the Loss function\n",
    "    #cross_entropy = -tf.reduce_sum(y_target*tf.log(y), name = 'cross_entropy')\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_target))\n",
    "    \n",
    "    # define optimization algorithm\n",
    "    #train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_target, 1))\n",
    "    # correct_prediction is list of boolean which is the result of comparing(model prediction , data)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "    # tf.cast() : changes true -> 1 / false -> 0\n",
    "    # tf.reduce_mean() : calculate the mean\n",
    "    \n",
    "    # create summary of parameters\n",
    "    tf.summary.histogram('weights_1', W_conv1)\n",
    "    tf.summary.histogram('y', y)\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(\"/tmp/cnn\")\n",
    "    \n",
    "    return accuracy, x, y_target, keep_prob, train_step, merged, y, cross_entropy, summary_writer\n",
    "    \n",
    "print(\"define cnn graph func\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_test : [[61.0, 1019.24, 1019.24, 22.1, 22.1, 22.1, 0.0, 1035.16], [79.0, 1016.77, 1016.77, 12.04, 12.04, 12.04, 0.0, 1032.94], [81.0, 1016.01, 1016.01, 13.68, 13.68, 13.68, 0.0, 1031.95], [88.0, 1018.84, 1018.84, 12.11, 12.11, 12.11, 0.0, 1035.03], [56.0, 1014.32, 1014.32, 22.7, 22.7, 22.7, 0.0, 1030.25], [57.0, 1020.96, 1020.96, 23.43, 23.43, 23.43, 0.0, 1036.87], [87.0, 1015.64, 1015.64, 10.35, 10.35, 10.35, 0.0, 1031.87], [56.0, 1019.72, 1019.72, 23.73, 23.73, 23.73, 0.0, 1035.68]]\n",
      "data_train : 29\n",
      "labels_train : 29\n",
      "data_test : 8\n",
      "labels_test : 8\n",
      "WARNING:tensorflow:From <ipython-input-149-c394928ba545>:12: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "step 0, training accuracy: 0.690\n",
      "step 10, training accuracy: 0.690\n",
      "step 20, training accuracy: 0.690\n",
      "step 30, training accuracy: 0.690\n",
      "step 40, training accuracy: 0.690\n",
      "step 50, training accuracy: 0.690\n",
      "step 60, training accuracy: 0.690\n",
      "step 70, training accuracy: 0.690\n",
      "step 80, training accuracy: 0.690\n",
      "step 90, training accuracy: 0.690\n",
      "test accuracy: 1\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "def run() : \n",
    "    try : \n",
    "        # get Data \n",
    "        labels_train, labels_test, data_filter_train, data_filter_test = get_test_data()\n",
    "        # reset Graph\n",
    "        tf.reset_default_graph()   \n",
    "        # Create Session\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))  \n",
    "        # create graph\n",
    "        accuracy, x, y_target, keep_prob, train_step, merged, y, cross_entropy, summary_writer = create_graph(train=True)\n",
    "        # set saver\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        # initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # training the MLP\n",
    "        for i in range(100): \n",
    "            sess.run(train_step, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 0.5})\n",
    "            if i%10 == 0:\n",
    "                train_accuracy = sess.run(accuracy, feed_dict={x:data_filter_train, y_target: labels_train, keep_prob: 1})\n",
    "                print (\"step %d, training accuracy: %.3f\"%(i, train_accuracy))\n",
    "                \n",
    "                # calculate the summary and write.\n",
    "                summary = sess.run(merged, feed_dict={x:data_filter_train, y_target: labels_train, keep_prob: 1})\n",
    "                summary_writer.add_summary(summary , i)\n",
    "                print(\"summary for tensorboard written\")\n",
    "                \n",
    "        # for given x, y_target data set\n",
    "        print  (\"test accuracy: %g\"% sess.run(accuracy, feed_dict={x:data_filter_test, y_target: labels_test, keep_prob: 1}))\n",
    "        \n",
    "        # Save Model\n",
    "        path = './model/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            print(\"path created\")\n",
    "        saver.save(sess, path)\n",
    "        print(\"model saved\")\n",
    "    except Exception as e : \n",
    "        raise Exception (\"error on training: {0}\".format(e))\n",
    "    finally :\n",
    "        sess.close()\n",
    "\n",
    "# run stuff\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/\n",
      "model restored\n",
      "input data : [63.0, 1017.55, 1017.55, 24.05, 24.05, 24.05, 0.0, 1033.55]\n",
      "result : [array([[ 1.,  0.,  0.]], dtype=float32)]\n",
      "result : 0\n"
     ]
    }
   ],
   "source": [
    "def predict(test_data) : \n",
    "    try : \n",
    "        # reset Graph\n",
    "        tf.reset_default_graph()   \n",
    "        # Create Session\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))  \n",
    "        # create graph\n",
    "        _, x, _, _, _, _, y, _, _ = create_graph(train=False)\n",
    "        \n",
    "        # initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # set saver\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        # Restore Model\n",
    "        path = './model/'\n",
    "        if os.path.exists(path):\n",
    "            saver.restore(sess, path)\n",
    "            print(\"model restored\")\n",
    "\n",
    "        # training the MLP\n",
    "        print(\"input data : {0}\".format(test_data))\n",
    "        y = sess.run([y], feed_dict={x: np.array([test_data])})\n",
    "        print(\"result : {0}\".format(y))\n",
    "        print(\"result : {0}\".format(np.argmax(y)))\n",
    "        \n",
    "    except Exception as e : \n",
    "        raise Exception (\"error on training: {0}\".format(e))\n",
    "    finally :\n",
    "        sess.close()\n",
    "\n",
    "# run stuff\n",
    "predict([63.0, 1017.55, 1017.55, 24.05, 24.05, 24.05, 0.0, 1033.55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
