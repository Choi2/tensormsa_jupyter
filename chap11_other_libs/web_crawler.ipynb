{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebCrawler Test\n",
    "아주 간단한 WebCralwer 를 테스트 해보았다. 간단하게 WikiPedia 한글 사이트를 처음으로 시작해서 해당 사이트에 존재하는 \"P\" 태그를 수집하고 해당 페이지에서 존재하는 Link를 찾아서 이동하고, 해당 페이지에서 \"P\" 태그를 찾아서 저장하는 행위를 반복하는 코드이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup 간단하게 연결해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>NAVER</title>\n"
     ]
    }
   ],
   "source": [
    "def crawler(iter) : \n",
    "    url = \"http://naver.com\"\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text , 'lxml')\n",
    "    for raw in soup.find_all('title') : \n",
    "        print(raw)\n",
    "crawler(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia 한글 페이지 Link를 따라가면서 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Job Start!!\n",
      "href : https://ko.wiktionary.org/wiki/\n",
      "href : https://ko.wiktionary.org/wiki/\n",
      "href : https://ko.wikinews.org/wiki/\n",
      "href : https://ko.wikinews.org/wiki/\n",
      "href : https://ko.wikisource.org/wiki/\n",
      "href : https://ko.wikisource.org/wiki/\n",
      "href : https://ko.wikiversity.org/wiki/\n",
      "href : https://ko.wikiversity.org/wiki/\n",
      "href : https://ko.wikivoyage.org/wiki/\n",
      "href : https://ko.wikivoyage.org/wiki/%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikiquote.org/wiki/\n",
      "href : https://ko.wikiquote.org/wiki/\n",
      "href : https://ko.wikibooks.org/wiki/\n",
      "href : https://ko.wikibooks.org/wiki/\n",
      "href : https://ko.wikipedia.org/w/index.php?title=위키백과:대문&oldid=15252069\n",
      "href : https://ko.wikibooks.org/wiki/%EC%9C%84%ED%82%A4%EC%B1%85:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikinews.org/wiki/%EC%9C%84%ED%82%A4%EB%89%B4%EC%8A%A4:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikiquote.org/wiki/%EC%9C%84%ED%82%A4%EC%9D%B8%EC%9A%A9%EC%A7%91:%EB%93%A4%EB%A8%B8%EB%A6%AC\n",
      "href : https://ko.wikisource.org/wiki/%EC%9C%84%ED%82%A4%EB%AC%B8%ED%97%8C:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikiversity.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B0%EC%9B%80%ED%84%B0:%EB%8C%80%EB%AC%B8\n",
      "# Job Done!!\n"
     ]
    }
   ],
   "source": [
    "def task(page, max_pages, url_path, file_w):\n",
    "    \"\"\"\n",
    "    지정된 수만큼 제귀 형태로 모든 링크를 따라가서 전부 수집한다. \n",
    "    \"\"\"\n",
    "    if page == max_pages :\n",
    "        get_single_article(url_path, file_w)\n",
    "    else : \n",
    "        source_code = requests.get(url_path)\n",
    "        plain_text = source_code.text\n",
    "        soup = BeautifulSoup(plain_text, 'lxml')\n",
    "        page += 1\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if (href != None and re.search(\"https://ko\", href)) : \n",
    "                print(\"href : {0}\".format(href))\n",
    "                task(page, max_pages, href, file_w)\n",
    "\n",
    "def get_single_article(item_url, file_w):\n",
    "    \"\"\"\n",
    "    p 태그를 가지고와서 파싱한다 \n",
    "    \"\"\"\n",
    "    source_code = requests.get(item_url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    for contents in soup.find_all('p'):\n",
    "        #print(\"url : {0} \".format(item_url))\n",
    "        #print(\"text : {0} \".format(contents.text))\n",
    "        file_w.write(contents.text)\n",
    "\n",
    "def spider(max_pages, url_path, path = \"/home/dev/wiki/\", file_name='test.txt') :\n",
    "    \"\"\"\n",
    "    본 Function 을 실행하면 WikiPedia 첫 페이지에서 실행해서 \n",
    "    지정된 횟수만큼 페이지를 따라 들어가서 정해진 패턴을 수집한다. \n",
    "    max_pages : 몇번 Page를 따라 들어갈 것인가를 정의하는 변수 \n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_w = open(''.join([path, file_name]), \"w\")  \n",
    "    print(\"# Job Start!!\")\n",
    "    task(1, max_pages, url_path, file_w)\n",
    "    print(\"# Job Done!!\")\n",
    "    file_w.close()\n",
    "\n",
    "# 주어진 횟수만큼 해당 사이트를 시작으로 크롤링 시작 \n",
    "# first parm : Inception 횟수 \n",
    "# second parm : initial site \n",
    "spider(2, 'https://ko.wikipedia.org/wiki/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 간단하게 Gensim Word2Vec 에 수집한 데이터 훈련\n",
    "Word2Vec 훈련 및 서비스는 Djnago REST Service 기반으로 구현해 보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : nn123\n",
      "evaluation result : nn123\n",
      "evaluation result : word2vec\n",
      "evaluation result : ['1 file upload success']\n",
      "evaluation result : {'source_server': 'local', 'source_sql': 'all', 'source_type': 'local', 'source_parse_type': 'raw', 'source_path': '/hoya_src_root/nn123/1/data_node', 'max_sentence_len': 10}\n",
      "evaluation result : mecab\n",
      "evaluation result : /hoya_str_root/nn123/1/data_node\n",
      "evaluation result : {'store_path': '/hoya_str_root/nn123/1/data_node', 'source_server': 'local', 'preprocess': 'mecab', 'source_sql': 'all', 'source_type': 'local', 'source_parse_type': 'raw', 'source_path': '/hoya_src_root/nn123/1/data_node', 'max_sentence_len': 10}\n",
      "evaluation result : {'batch_size': 100, 'iter': 5, 'window_size': 5, 'model_path': '/hoya_model_root/nn123/1/netconf_node', 'min_count': 1, 'vector_size': 100}\n",
      "evaluation result : ['1 file upload success']\n",
      "evaluation result : {'source_server': 'local', 'source_sql': 'all', 'source_type': 'local', 'source_parse_type': 'raw', 'source_path': '/hoya_src_root/nn123/1/test_data_node', 'max_sentence_len': 50}\n",
      "evaluation result : mecab\n",
      "evaluation result : /hoya_str_root/nn123/1/test_data_node\n",
      "evaluation result : {'result': \"'TrainSummaryInfo' object has no attribute 'result_info'\", 'status': '404'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json, os\n",
    "\n",
    "url = \"{0}:{1}\".format(os.environ['HOSTNAME'] , \"8000\")\n",
    "nn_id = \"nn123\"\n",
    "nn_wf_ver_id =\"1\"\n",
    "\n",
    "# Seq - 1\n",
    "resp = requests.post('http://' + url + '/api/v1/type/common/target/nninfo/nnid/' + nn_id + '/',\n",
    "                     json={\n",
    "                         \"biz_cate\": \"MES\",\n",
    "                         \"biz_sub_cate\": \"M60\",\n",
    "                         \"nn_title\" : \"test\",\n",
    "                         \"nn_desc\": \"test desc\",\n",
    "                         \"use_flag\" : \"Y\",\n",
    "                         \"dir\": \"purpose?\",\n",
    "                         \"config\": \"N\"\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"1.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 2\n",
    "resp = requests.post('http://' + url + '/api/v1/type/common/target/nninfo/nnid/' + nn_id + '/version/',\n",
    "                 json={\n",
    "                     \"nn_def_list_info_nn_id\": \"\",\n",
    "                     \"nn_wf_ver_info\": \"test version info\",\n",
    "                     \"condition\": \"1\",\n",
    "                     \"active_flag\": \"Y\"\n",
    "                 })\n",
    "data = json.loads(resp.json())\n",
    "print(\"2.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 3\n",
    "resp = requests.post('http://' + url + '/api/v1/type/wf/target/init/mode/simple/'+ nn_id + '/wfver/1/',\n",
    "                     json={\n",
    "                         \"type\": \"word2vec\"\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"3.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 4\n",
    "return_dict = {}\n",
    "return_dict['test'] = open('/home/dev/wiki/test.txt', 'rb')\n",
    "\n",
    "resp = requests.post('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/source/nnid/'+nn_id+'/ver/1/node/data_node/',\n",
    "                     files = return_dict)\n",
    "\n",
    "data = json.loads(resp.json())\n",
    "print(\"4.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 5\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/source/nnid/'+ nn_id + '/ver/1/node/data_node/',\n",
    "                     json={\n",
    "                         \"source_server\": \"local\",\n",
    "                         \"source_sql\": \"all\",\n",
    "                         \"max_sentence_len\" : 10\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"5.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 6\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/pre/nnid/'+ nn_id + '/ver/1/node/data_node/',\n",
    "                     json={\n",
    "                         \"preprocess\":  \"mecab\",\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"6.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 7\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/store/nnid/'+ nn_id + '/ver/1/node/data_node/')\n",
    "data = json.loads(resp.json())\n",
    "print(\"7.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 8\n",
    "resp = requests.get('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/source/nnid/'+ nn_id + '/ver/1/node/data_node/')\n",
    "data = json.loads(resp.json())\n",
    "print(\"8.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 9\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/netconf/detail/w2v/nnid/' + nn_id + '/ver/' + nn_wf_ver_id + '/node/netconf_node/',\n",
    "                     json={\n",
    "                        \"model_path\" : \"test\",\n",
    "                        \"window_size\" : 5,\n",
    "                        \"vector_size\" : 100,\n",
    "                        \"batch_size\" : 100,\n",
    "                        \"iter\" : 5,\n",
    "                        \"min_count\" : 1\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"9.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 10\n",
    "return_dict = {}\n",
    "return_dict['test'] = open('/home/dev/wiki/test.txt', 'rb')\n",
    "resp = requests.post('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/source/nnid/'+ nn_id + '/ver/1/node/test_data_node/',\n",
    "                     files = return_dict)\n",
    "data = json.loads(resp.json())\n",
    "print(\"10.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 11\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/source/nnid/'+ nn_id + '/ver/1/node/test_data_node/',\n",
    "                     json={\n",
    "                         \"source_server\": \"local\",\n",
    "                         \"source_sql\": \"all\",\n",
    "                         \"max_sentence_len\" : 50\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"11.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 12\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/pre/nnid/'+ nn_id + '/ver/1/node/test_data_node/',\n",
    "                     json={\n",
    "                         \"preprocess\":  \"mecab\",\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"12.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 13\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/textdata/src/local/form/raw/prg/store/nnid/'+ nn_id + '/ver/1/node/test_data_node/')\n",
    "data = json.loads(resp.json())\n",
    "print(\"13.evaluation result : {0}\".format(data))\n",
    "\n",
    "# Seq - 14\n",
    "resp = requests.post('http://' + url + '/api/v1/type/runmanager/state/train/nnid/'+nn_id+'/ver/'+nn_wf_ver_id+'/')\n",
    "data = json.loads(resp.json())\n",
    "print(\"14.evaluation result : {0}\".format(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : [[['침해/NNG', 0.02471858263015747], ['사전/NNG', 0.022121289744973183], ['도울/VV+ETM', 0.015608951449394226], ['모두/NNG', 0.015334065072238445], ['세요/EP+EF', 0.015333017334342003]]]\n"
     ]
    }
   ],
   "source": [
    "# Run All Workflow\n",
    "resp = requests.post('http://' + url + '/api/v1/type/service/state/predict/type/w2v/nnid/' + nn_id + '/ver/active/',\n",
    "                     json={\n",
    "                         \"type\": \"sim\",\n",
    "                         \"val_1\":[\"포털\"],\n",
    "                         \"val_2\":[\"윤리\"]\n",
    "                     }\n",
    "                     )\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
