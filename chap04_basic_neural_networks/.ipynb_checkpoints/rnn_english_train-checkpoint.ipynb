{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recurrent Neural Network - Train\n",
    "간단하게 FCNN 과 CNN 을 설명하였다. RNN 은 Recurrent Neural Network 의 약자로 순차적인 데이터를 처리하기 위하여 고안이 되었다.  \n",
    "예를들면 자연어 데이터 같은 것이 있을 수 있다. 본 예제는 https://github.com/sjchoi86/Tensorflow-101 을 참조하여 작성되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H3>(1) Package Load</H3>   \n",
    "아무래도 RNN 은 데이터 전처리 작업이 많다보니 여러가지 Package 들을 사용하게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "print (\"Packages Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>(3) Data Load</H3>   \n",
    "데이터가 잘 Load 되었는지 확인한다. Sample Data 는 C 프로그램으로 보인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============load file\n",
      "Text loaded from 'RNN_data/input.txt'\n",
      "=============file contents\n",
      "Text Length : * SUSPEND;\n",
      "\tresume = sbuf.f_blocks * RESUME;\n",
      "\n",
      "\tsector_div(suspend, 100);\n",
      "\tsector_div(resume, 100);\n",
      "\n",
      "\tif (sbuf.f_bavail <= suspend)\n",
      "\t\tact = -1;\n",
      "\telse if (sbuf.f_bavail >= resume)\n",
      "\t\tact = 1;\n",
      "\telse\n",
      "\t\tact = 0;\n",
      "\n",
      "\t/*\n",
      "\t * If some joker switched acct_globals.file under us we'ld better be\n",
      "\t * silent and _not_ touch anything.\n",
      "\t */\n",
      "\tspin_lock(&acct_globals.lock);\n",
      "\tif (file != acct_globals.file) {\n",
      "\t\tif (act)\n",
      "\t\t\tres = act>0;\n",
      "\t\tgoto out;\n",
      "\t}\n",
      "\n",
      "\tif (acct_globals.active) {\n",
      "\t\tif (act < 0) {\n",
      "\t\t\tacct_globals.active \n",
      "=============file length\n",
      "Text Length : 1708864\n"
     ]
    }
   ],
   "source": [
    "# Load text\n",
    "# data_dir    = \"data/tinyshakespeare\"\n",
    "data_dir    = \"RNN_data/\"\n",
    "save_dir    = \"RNN_data/\"\n",
    "input_file  = os.path.join(data_dir, \"input.txt\")\n",
    "with open(input_file, \"r\") as f:\n",
    "    data = f.read()\n",
    "print(\"=============load file\")\n",
    "print(\"Text loaded from '%s'\" % (input_file))\n",
    "print(\"=============file contents\")\n",
    "print(\"Text Length : {0}\".format(data[4000:4500]))\n",
    "print(\"=============file length\")\n",
    "print(\"Text Length : {0}\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>(4) Distinct, Sort</H3>   \n",
    "Contents 에 사용된 Distinct 한 단어와 Count 횟수를 Pair 하게 Mapping 하고 Count 역순으로 정렬한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'counter.items()' is <class 'dict_items'> and length is 98\n",
      "[0/3]\n",
      "('v', 9074)\n",
      "[1/3]\n",
      "('h', 25480)\n",
      "[2/3]\n",
      "('.', 8860)\n",
      "[3/3]\n",
      "('8', 190)\n",
      "[4/3]\n",
      "('$', 61)\n",
      " \n",
      "Type of 'count_pairs' is <class 'list'> and length is 98\n",
      "[0/3]\n",
      "(' ', 171219)\n",
      "[1/3]\n",
      "('e', 113021)\n",
      "[2/3]\n",
      "('t', 102154)\n",
      "[3/3]\n",
      "('r', 76185)\n",
      "[4/3]\n",
      "('i', 75487)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Text\n",
    "# First, count the number of characters\n",
    "counter = collections.Counter(data)\n",
    "count_pairs = sorted(counter.items(), key=lambda x: -x[1]) # <= Sort\n",
    "\n",
    "print (\"Type of 'counter.items()' is %s and length is %d\" \n",
    "       % (type(counter.items()), len(counter.items()))) \n",
    "for i in range(5):\n",
    "    print (\"[%d/%d]\" % (i, 3)), # <= This comma remove '\\n'\n",
    "    print (list(counter.items())[i])\n",
    "\n",
    "print (\" \")\n",
    "print (\"Type of 'count_pairs' is %s and length is %d\" \n",
    "       % (type(count_pairs), len(count_pairs))) \n",
    "for i in range(5):\n",
    "    print (\"[%d/%d]\" % (i, 3)), # <= This comma remove '\\n'\n",
    "    print (count_pairs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>(5) vocab & chars</H3>   \n",
    "vacab 을 사용하여 Char 의 Index 를 구할 수 있으며, chars 를 사용하여 Index 로 Char 를 구할 수 있다.  \n",
    "Voca Type 과 Voca Contents 로 데이터를 구성하여 voca file 을 만들어 저장한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'chars' is <class 'tuple'> and length is 98\n",
      "[0/3]\n",
      "chars[0] is ' '\n",
      "[1/3]\n",
      "chars[1] is 'e'\n",
      "[2/3]\n",
      "chars[2] is 't'\n",
      "[3/3]\n",
      "chars[3] is 'r'\n",
      "[4/3]\n",
      "chars[4] is 'i'\n",
      "\n",
      "Type of 'vocab' is <class 'dict'> and length is 98\n",
      "[0/3]\n",
      "vocab[' '] is 0\n",
      "[1/3]\n",
      "vocab['e'] is 1\n",
      "[2/3]\n",
      "vocab['t'] is 2\n",
      "[3/3]\n",
      "vocab['r'] is 3\n",
      "[4/3]\n",
      "vocab['i'] is 4\n"
     ]
    }
   ],
   "source": [
    "# Let's make dictionary\n",
    "chars, counts = zip(*count_pairs)\n",
    "\n",
    "vocab = dict(zip(chars, range(len(chars))))\n",
    "\n",
    "print (\"Type of 'chars' is %s and length is %d\" \n",
    "    % (type(chars), len(chars))) \n",
    "for i in range(5):\n",
    "    print (\"[%d/%d]\" % (i, 3)), # <= This comma remove '\\n'\n",
    "    print (\"chars[%d] is '%s'\" % (i, chars[i]))\n",
    "    \n",
    "print (\"\")\n",
    "\n",
    "print (\"Type of 'vocab' is %s and length is %d\" \n",
    "    % (type(vocab), len(vocab))) \n",
    "for i in range(5):\n",
    "    print (\"[%d/%d]\" % (i, 3)), # <= This comma remove '\\n'\n",
    "    print (\"vocab['%s'] is %s\" % (chars[i], vocab[chars[i]]))\n",
    "    \n",
    "# SAve chars and vocab\n",
    "with open(os.path.join(save_dir, 'chars_vocab.pkl'), 'wb') as f:\n",
    "    cPickle.dump((chars, vocab), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>(6) Corpus</H3>   \n",
    "위에서 생성한 vocab (char : 순번) 형태의 Dict 데이터를 활용하여 훈련하고자 하는 전문을 [ 8 14 40 36 24  1  3  8  1 13] 형태로 바꾼다 로그를 보면 원하는 대로 잘 변환이 된 것을 확일 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'corpus' is <class 'numpy.ndarray'>, shape is (1708864,), and length is 1708864\n",
      "\n",
      "'corpus' looks like [ 8 14 40 36 24  1  3  8  1 13]\n",
      "[0/10] chars[08] corresponds to 'n'\n",
      "[1/10] chars[14] corresponds to 'u'\n",
      "[2/10] chars[40] corresponds to 'x'\n",
      "[3/10] chars[36] corresponds to '/'\n",
      "[4/10] chars[24] corresponds to 'k'\n",
      "[5/10] chars[01] corresponds to 'e'\n",
      "[6/10] chars[03] corresponds to 'r'\n",
      "[7/10] chars[08] corresponds to 'n'\n",
      "[8/10] chars[01] corresponds to 'e'\n",
      "[9/10] chars[13] corresponds to 'l'\n"
     ]
    }
   ],
   "source": [
    "# Now convert all text to index using vocab! \n",
    "corpus = np.array(list(map(vocab.get, data)))\n",
    "print (\"Type of 'corpus' is %s, shape is %s, and length is %d\" \n",
    "    % (type(corpus), corpus.shape, len(corpus)))\n",
    "\n",
    "check_len = 10\n",
    "print (\"\\n'corpus' looks like %s\" % (corpus[0:check_len]))\n",
    "for i in range(check_len):\n",
    "    _wordidx = corpus[i]\n",
    "    print (\"[%d/%d] chars[%02d] corresponds to '%s'\" \n",
    "           % (i, check_len, _wordidx, chars[_wordidx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>(7) Prepare Train Data</H3>   \n",
    "Encode 200 , Decode 200 인 Sequence2Sequece Model 을 위한 Data 를 구성한다. 원래 데이터의 길이는 1708864 인데 자투리는 정리하고 1700000 사이즈로 만들어 주고 XData와  YData 를 구성한다. Ydata (Decode) 데이터는 XData 와 같은 데이터를 사용하되 한 칸씩 데이터를 밀어준다.   \n",
    "XData가 [a,b,c,d,e] 라면 YData 는 [b,c,d,e,f] 식으로 말이다.  \n",
    "최종적으로 구성되는 데이터는 각 XData 와 YData 에 대하여 200 X 50 Matrix 가 170개 있는 형태로 구성이 될 것이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xdata is ... [ 8 14 40 ...,  3  1  1] and length is 1700000\n",
      "ydata is ... [14 40 36 ...,  1  1  8] and length is 1700000\n",
      "\n",
      "Type of 'xbatches' is <class 'list'> and length is 170\n",
      "Type of 'ybatches' is <class 'list'> and length is 170\n",
      "[[16 26  0 ..., 10 13 11]\n",
      " [ 3  4  8 ..., 13  4 19]\n",
      " [18 20 23 ..., 24 21 46]\n",
      " ..., \n",
      " [40 10 15 ...,  4  2  1]\n",
      " [ 7  6 12 ...,  8 13  4]\n",
      " [ 4  5  2 ...,  3  1  1]]\n",
      "\n",
      "Type of 'temp' is <class 'list'> and length is 5\n",
      "Type of 'temp[0]' is <class 'numpy.ndarray'> and shape is (50, 200)\n",
      "Type of 'temp[1]' is <class 'numpy.ndarray'> and shape is (50, 200)\n",
      "Type of 'temp[2]' is <class 'numpy.ndarray'> and shape is (50, 200)\n",
      "Type of 'temp[3]' is <class 'numpy.ndarray'> and shape is (50, 200)\n",
      "Type of 'temp[4]' is <class 'numpy.ndarray'> and shape is (50, 200)\n"
     ]
    }
   ],
   "source": [
    "# Generate batch data \n",
    "batch_size  = 50\n",
    "seq_length  = 200\n",
    "num_batches = int(corpus.size / (batch_size * seq_length))\n",
    "# First, reduce the length of corpus to fit batch_size\n",
    "corpus_reduced = corpus[:(num_batches*batch_size*seq_length)]\n",
    "xdata = corpus_reduced\n",
    "ydata = np.copy(xdata)\n",
    "ydata[:-1] = xdata[1:]\n",
    "ydata[-1]  = xdata[0]\n",
    "\n",
    "print ('xdata is ... %s and length is %d' % (xdata, xdata.size))\n",
    "print ('ydata is ... %s and length is %d' % (ydata, xdata.size))\n",
    "print (\"\")\n",
    "\n",
    "# Second, make batch \n",
    "xbatches = np.split(xdata.reshape(batch_size, -1), num_batches, 1)\n",
    "ybatches = np.split(ydata.reshape(batch_size, -1), num_batches, 1)\n",
    "print (\"Type of 'xbatches' is %s and length is %d\" \n",
    "    % (type(xbatches), len(xbatches)))\n",
    "print (\"Type of 'ybatches' is %s and length is %d\" \n",
    "    % (type(ybatches), len(ybatches)))\n",
    "print(xbatches[169])\n",
    "print (\"\")\n",
    "\n",
    "\n",
    "# How can we access to xbatches?? \n",
    "nbatch = 5\n",
    "temp = xbatches[0:nbatch]\n",
    "print (\"Type of 'temp' is %s and length is %d\" \n",
    "    % (type(temp), len(temp)))\n",
    "for i in range(nbatch):\n",
    "    temp2 = temp[i]\n",
    "    print (\"Type of 'temp[%d]' is %s and shape is %s\" % (i, type(temp2), temp2.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>(8) Net Config </H3>   \n",
    "RNN 은 전처리가 많다보니 이제.. net config 부분이다..  RNN 은 크게 Vanlia, LSTM, GRU 정도로 나눌 수 있는데, Tensorflow 에서 구현시에는 함수 명만 달라지는 정도로 나머지는 Tensorflow 가 알아서 해준다.. Vanila 의 경우 간단한 예제에서만 사용하는 정도로 Vanishing Problem 때문에 사용하지 않는다.. 여기서는 LSTM 을 사용하는 예제로 모델 구성은 그림을 그리면 간단할태지만 말로 설명하자면 각 Cell 은 LSTM 으로 구성되어 있고 Depth는 2 , 그리고 Cell 의 Size 는 128 로 구성되어 있으며, Encode 부와 Decode 부는 각 200인 형태의 RNN 모델이 되겠다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Ready\n"
     ]
    }
   ],
   "source": [
    "# Important RNN parameters \n",
    "vocab_size = len(vocab)\n",
    "rnn_size   = 128\n",
    "num_layers = 2\n",
    "grad_clip  = 5.\n",
    "\n",
    "\n",
    "# Construct RNN model \n",
    "unitcell   = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
    "cell       = tf.nn.rnn_cell.MultiRNNCell([unitcell] * num_layers)\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets    = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "istate     = cell.zero_state(batch_size, tf.float32)\n",
    "# Weigths \n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "        inputs = tf.split(1, seq_length, tf.nn.embedding_lookup(embedding, input_data))\n",
    "        inputs = [tf.squeeze(_input, [1]) for _input in inputs]\n",
    "# Output\n",
    "def loop(prev, _):\n",
    "    prev = tf.nn.xw_plus_b(prev, softmax_w, softmax_b)\n",
    "    prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "    return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\"\"\"\n",
    "    loop_function: If not None, this function will be applied to the i-th output\n",
    "    in order to generate the i+1-st input, and decoder_inputs will be ignored,\n",
    "    except for the first element (\"GO\" symbol).\n",
    "\"\"\" \n",
    "outputs, last_state = tf.nn.seq2seq.rnn_decoder(inputs, istate, cell\n",
    "                , loop_function=None, scope='rnnlm')\n",
    "output = tf.reshape(tf.concat(1, outputs), [-1, rnn_size])\n",
    "logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "probs  = tf.nn.softmax(logits)\n",
    "# Loss\n",
    "loss = tf.nn.seq2seq.sequence_loss_by_example([logits], # Input\n",
    "    [tf.reshape(targets, [-1])], # Target\n",
    "    [tf.ones([batch_size * seq_length])], # Weight \n",
    "    vocab_size)\n",
    "# Optimizer\n",
    "cost     = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "final_state = last_state\n",
    "lr       = tf.Variable(0.0, trainable=False)\n",
    "tvars    = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "_optm    = tf.train.AdamOptimizer(lr)\n",
    "optm     = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "print (\"Network Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>(9) Session 실행 </H3>   \n",
    "갱신된 State 를 받아서 다시 입력으로 State 를 넣어 주는 부분을 제외하고는 특이한 부분은 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-1c0c5b1200c1>:8 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-1c0c5b1200c1>:9 in <module>.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "WARNING:tensorflow:From <ipython-input-8-1c0c5b1200c1>:10 in <module>.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "==== xbatch : [[ 8 14 40 ...,  1  3  5]\n",
      " [12 24 21 ..., 27  9  3]\n",
      " [ 4  2  0 ...,  2 21 52]\n",
      " ..., \n",
      " [49 48 50 ..., 20 23  7]\n",
      " [ 3 21 32 ...,  1  0 15]\n",
      " [ 8  1 10 ...,  8  0 38]]\n",
      "==== ybatch : [[14 40 36 ...,  3  5 35]\n",
      " [24 21 46 ...,  9  3  3]\n",
      " [ 2  0  1 ..., 21 52 11]\n",
      " ..., \n",
      " [48 50 10 ..., 23  7  6]\n",
      " [21 32 20 ...,  0 15  1]\n",
      " [ 1 10 19 ...,  0 38 47]]\n",
      "[0/8500] cost: 5.2934 / Each batch learning took 2.9860 sec\n",
      "model saved to 'RNN_data/model.ckpt'\n",
      "==== xbatch : [[ 8 13  4 ...,  0 22 36]\n",
      " [38 53 45 ..., 33 45 26]\n",
      " [ 6  4 17 ..., 10 50 33]\n",
      " ..., \n",
      " [ 9  4  2 ...,  0 12 14]\n",
      " [ 3  3 23 ..., 29  0 30]\n",
      " [ 2  5  9 ..., 14 16  2]]\n",
      "==== ybatch : [[13  4  8 ..., 22 36  7]\n",
      " [53 45 45 ..., 45 26  0]\n",
      " [ 4 17  0 ..., 50 33 41]\n",
      " ..., \n",
      " [ 4  2  1 ..., 12 14  3]\n",
      " [ 3 23  7 ...,  0 30  4]\n",
      " [ 5  9 12 ..., 16  2  4]]\n",
      "[100/8500] cost: 3.1643 / Each batch learning took 0.6600 sec\n",
      "==== xbatch : [[ 1  8 21 ..., 17  4 13]\n",
      " [ 4  8 24 ..., 42  0  9]\n",
      " [10 17 11 ..., 18  1  3]\n",
      " ..., \n",
      " [30 35  7 ..., 14 12  2]\n",
      " [ 4 17  0 ..., 26  0 52]\n",
      " [ 1 15  0 ..., 12 11 19]]\n",
      "==== ybatch : [[ 8 21  8 ...,  4 13  1]\n",
      " [ 8 24  0 ...,  0  9  2]\n",
      " [17 11  3 ...,  1  3  1]\n",
      " ..., \n",
      " [35  7  6 ..., 12  2  0]\n",
      " [17  0 21 ...,  0 52  0]\n",
      " [15  0  2 ..., 11 19 16]]\n",
      "[200/8500] cost: 2.5925 / Each batch learning took 0.6597 sec\n",
      "==== xbatch : [[27 14 17 ..., 17 28 27]\n",
      " [ 5  1  0 ..., 13  1 15]\n",
      " [67 20  0 ..., 37 74 50]\n",
      " ..., \n",
      " [15 28 31 ..., 19 14  2]\n",
      " [13 11  9 ...,  0  2 18]\n",
      " [41 33 58 ...,  1  2 34]]\n",
      "==== ybatch : [[14 17 17 ..., 28 27  9]\n",
      " [ 1  0 48 ...,  1 15  0]\n",
      " [20  0 43 ..., 74 50 33]\n",
      " ..., \n",
      " [28 31 11 ..., 14  2  1]\n",
      " [11  9 15 ...,  2 18  9]\n",
      " [33 58 50 ...,  2 34  9]]\n",
      "[300/8500] cost: 2.4404 / Each batch learning took 0.6656 sec\n",
      "==== xbatch : [[68 85  0 ..., 15  0 13]\n",
      " [ 0  9  3 ...,  9  2  1]\n",
      " [20 23  0 ..., 18 20 23]\n",
      " ..., \n",
      " [ 8  2  0 ...,  2 18  4]\n",
      " [ 4 13  1 ..., 35  8  3]\n",
      " [19 27 21 ...,  8  1 10]]\n",
      "==== ybatch : [[85  0 22 ...,  0 13 11]\n",
      " [ 9  3  1 ...,  2  1 26]\n",
      " [23  0 36 ..., 20 23  7]\n",
      " ..., \n",
      " [ 2  0 11 ..., 18  4  8]\n",
      " [13  1 15 ...,  8  3 10]\n",
      " [27 21 20 ...,  1 10  2]]\n",
      "[400/8500] cost: 2.2298 / Each batch learning took 0.6645 sec\n",
      "==== xbatch : [[66 66 84 ...,  1  0 29]\n",
      " [ 0 22  0 ..., 11  8  0]\n",
      " [16  9  3 ...,  3 30  0]\n",
      " ..., \n",
      " [ 8 11  3 ..., 37 59  0]\n",
      " [36 22  0 ...,  8  2 24]\n",
      " [ 5  1  3 ..., 12  2 13]]\n",
      "==== ybatch : [[66 84 60 ...,  0 29  0]\n",
      " [22  0  9 ...,  8  0  1]\n",
      " [ 9  3  1 ..., 30  0 22]\n",
      " ..., \n",
      " [11  3 19 ..., 59  0 80]\n",
      " [22  0 76 ...,  2 24 21]\n",
      " [ 1  3  0 ...,  2 13 10]]\n",
      "[500/8500] cost: 1.9722 / Each batch learning took 0.6692 sec\n",
      "model saved to 'RNN_data/model.ckpt'\n",
      "==== xbatch : [[ 1  1 10 ...,  8  2 26]\n",
      " [ 0 36 22 ..., 25  1  2]\n",
      " [11  0  1 ...,  0 66  0]\n",
      " ..., \n",
      " [10 47 38 ..., 46 12 14]\n",
      " [40  2  0 ...,  0  9  3]\n",
      " [30  5 10 ..., 12 11  8]]\n",
      "==== ybatch : [[ 1 10  4 ...,  2 26  0]\n",
      " [36 22  0 ...,  1  2 26]\n",
      " [ 0  1 40 ..., 66  0 48]\n",
      " ..., \n",
      " [47 38 21 ..., 12 14  3]\n",
      " [ 2  0 51 ...,  9  3  3]\n",
      " [ 5 10  5 ..., 11  8 15]]\n",
      "[600/8500] cost: 1.9352 / Each batch learning took 0.6594 sec\n",
      "==== xbatch : [[22  0 42 ...,  9 12  2]\n",
      " [ 0 39  8 ..., 18  1  9]\n",
      " [ 0  9 14 ..., 15 11  8]\n",
      " ..., \n",
      " [12 13 11 ..., 11 40 30]\n",
      " [ 8  0 28 ..., 50 45 39]\n",
      " [11 19 16 ...,  4  2  4]]\n",
      "==== ybatch : [[ 0 42 53 ..., 12  2  0]\n",
      " [39  8 12 ...,  1  9 15]\n",
      " [ 9 14 15 ..., 11  8 71]\n",
      " ..., \n",
      " [13 11  8 ..., 40 30  0]\n",
      " [ 0 28  8 ..., 45 39 37]\n",
      " [19 16 13 ...,  2  4 12]]\n",
      "[700/8500] cost: 1.7571 / Each batch learning took 0.6672 sec\n",
      "==== xbatch : [[12 18 14 ...,  9  3  1]\n",
      " [26  0 22 ..., 26  0  5]\n",
      " [ 0 17 28 ...,  5 32  4]\n",
      " ..., \n",
      " [ 0 44  0 ..., 40  1  5]\n",
      " [68 33 56 ...,  0  1 40]\n",
      " [10 12  9 ..., 22  0  9]]\n",
      "==== ybatch : [[18 14  8 ...,  3  1  0]\n",
      " [ 0 22  8 ...,  0  5  4]\n",
      " [17 28 31 ..., 32  4  2]\n",
      " ..., \n",
      " [44  0 32 ...,  1  5  7]\n",
      " [33 56 42 ...,  1 40 16]\n",
      " [12  9 13 ...,  0  9 34]]\n",
      "[800/8500] cost: 1.7487 / Each batch learning took 0.6614 sec\n",
      "==== xbatch : [[ 1 40 16 ...,  9  3 25]\n",
      " [ 2 20 23 ...,  8 11 32]\n",
      " [ 2 12 18 ...,  9  2  9]\n",
      " ..., \n",
      " [11 34  1 ..., 39 38 37]\n",
      " [21 13 11 ..., 34 12  5]\n",
      " [ 9 12 18 ...,  4 12 18]]\n",
      "==== ybatch : [[40 16 67 ...,  3 25  1]\n",
      " [20 23  7 ..., 11 32 23]\n",
      " [12 18 35 ...,  2  9  0]\n",
      " ..., \n",
      " [34  1  3 ..., 38 37 10]\n",
      " [13 11  8 ..., 12  5 32]\n",
      " [12 18  4 ..., 12 18  0]]\n",
      "[900/8500] cost: 1.6933 / Each batch learning took 0.6631 sec\n",
      "==== xbatch : [[ 6  6 16 ..., 14  8 15]\n",
      " [26  0 25 ...,  8 13 19]\n",
      " [35  0 22 ..., 12 18  9]\n",
      " ..., \n",
      " [21 24  2 ...,  0 22  0]\n",
      " [23  7  5 ...,  2  3 20]\n",
      " [14 12  2 ..., 15  0 17]]\n",
      "==== ybatch : [[ 6 16 14 ...,  8 15  1]\n",
      " [ 0 25 17 ..., 13 19  5]\n",
      " [ 0 22 36 ..., 18  9  3]\n",
      " ..., \n",
      " [24  2 18 ..., 22  0 12]\n",
      " [ 7  5  2 ...,  3 20  7]\n",
      " [12  2  0 ...,  0 17  3]]\n",
      "[1000/8500] cost: 1.5830 / Each batch learning took 0.6604 sec\n",
      "model saved to 'RNN_data/model.ckpt'\n",
      "==== xbatch : [[11 12 24 ..., 14  8 24]\n",
      " [ 0 34 11 ..., 23  7  7]\n",
      " [21  4  8 ..., 53 56 39]\n",
      " ..., \n",
      " [76 38 33 ...,  5  4 25]\n",
      " [ 7 44  7 ...,  3 51 26]\n",
      " [30  5 12 ...,  3 11 27]]\n",
      "==== ybatch : [[12 24 21 ...,  8 24 23]\n",
      " [34 11  4 ...,  7  7  6]\n",
      " [ 4  8  2 ..., 56 39 37]\n",
      " ..., \n",
      " [38 33 41 ...,  4 25  8]\n",
      " [44  7  7 ..., 51 26  0]\n",
      " [ 5 12  9 ..., 11 27 14]]\n",
      "[1100/8500] cost: 1.5778 / Each batch learning took 0.6606 sec\n",
      "==== xbatch : [[36 72 54 ..., 15  1  0]\n",
      " [21 46 18 ...,  4 15  0]\n",
      " [44  7  7 ...,  2 10 13]\n",
      " ..., \n",
      " [ 0  8  5 ..., 14  8  2]\n",
      " [ 1  9 15 ...,  2  1 20]\n",
      " [12  2  7 ..., 16 28 31]]\n",
      "==== ybatch : [[72 54 54 ...,  1  0 66]\n",
      " [46 18  9 ..., 15  0 18]\n",
      " [ 7  7 36 ..., 10 13 11]\n",
      " ..., \n",
      " [ 8  5 16 ...,  8  2 26]\n",
      " [ 9 15 21 ...,  1 20 23]\n",
      " [ 2  7  0 ..., 28 31 16]]\n",
      "[1200/8500] cost: 1.4157 / Each batch learning took 0.6556 sec\n",
      "==== xbatch : [[ 1 23  0 ..., 70 35  4]\n",
      " [15 29 80 ...,  6  6  6]\n",
      " [39 56 59 ...,  0 48 53]\n",
      " ..., \n",
      " [23  7  6 ...,  1 10  1]\n",
      " [11 27 14 ..., 15 28 31]\n",
      " [ 4 17  0 ..., 23  7  6]]\n",
      "==== ybatch : [[23  0  4 ..., 35  4  8]\n",
      " [29 80 14 ...,  6  6  6]\n",
      " [56 59  7 ..., 48 53 56]\n",
      " ..., \n",
      " [ 7  6 16 ..., 10  1 19]\n",
      " [27 14  5 ..., 28 31  5]\n",
      " [17  0 21 ...,  7  6  5]]\n",
      "[1300/8500] cost: 1.5611 / Each batch learning took 0.6521 sec\n",
      "==== xbatch : [[ 4 13  1 ..., 17 10 16]\n",
      " [ 0 56 11 ...,  0  2 18]\n",
      " [16  9  8 ...,  1  0 57]\n",
      " ..., \n",
      " [ 8 13 11 ...,  9  8 15]\n",
      " [ 0 22 34 ...,  1 21 20]\n",
      " [15 10 14 ...,  8 35  0]]\n",
      "==== ybatch : [[13  1  0 ..., 10 16  9]\n",
      " [56 11  4 ...,  2 18  1]\n",
      " [ 9  8 30 ...,  0 57  3]\n",
      " ..., \n",
      " [13 11 12 ...,  8 15  0]\n",
      " [22 34 20 ..., 21 20 28]\n",
      " [10 14  8 ..., 35  0  0]]\n",
      "[1400/8500] cost: 1.4815 / Each batch learning took 0.6587 sec\n",
      "==== xbatch : [[12 18 14 ...,  6 24  4]\n",
      " [24 28 31 ..., 27  1  0]\n",
      " [39 45 37 ...,  1  2 14]\n",
      " ..., \n",
      " [ 2 18  1 ...,  2 15 28]\n",
      " [ 0 38 41 ...,  8  2  0]\n",
      " [ 5  4 63 ...,  4  8 17]]\n",
      "==== ybatch : [[18 14  8 ..., 24  4 13]\n",
      " [28 31  5 ...,  1  0  4]\n",
      " [45 37 33 ...,  2 14  3]\n",
      " ..., \n",
      " [18  1  0 ..., 15 28 31]\n",
      " [38 41 10 ...,  2  0 19]\n",
      " [ 4 63  1 ...,  8 17 11]]\n",
      "[1500/8500] cost: 1.4775 / Each batch learning took 0.6550 sec\n",
      "model saved to 'RNN_data/model.ckpt'\n",
      "==== xbatch : [[28 31  3 ..., 14 12  2]\n",
      " [43  7  6 ...,  2 13  4]\n",
      " [ 7  6  1 ...,  7  6 12]\n",
      " ..., \n",
      " [66 13  4 ...,  2  0 32]\n",
      " [ 5  2  3 ..., 10  4 15]\n",
      " [ 8 15  0 ...,  2 30  0]]\n",
      "==== ybatch : [[31  3 13 ..., 12  2  0]\n",
      " [ 7  6  5 ..., 13  4  8]\n",
      " [ 6  1  8 ...,  6 12 18]\n",
      " ..., \n",
      " [13  4  8 ...,  0 32  9]\n",
      " [ 2  3 14 ...,  4 15 13]\n",
      " [15  0  2 ..., 30  0 29]]\n",
      "[1600/8500] cost: 1.4824 / Each batch learning took 0.6662 sec\n",
      "==== xbatch : [[ 8 14 40 ...,  1  3  5]\n",
      " [12 24 21 ..., 27  9  3]\n",
      " [ 4  2  0 ...,  2 21 52]\n",
      " ..., \n",
      " [49 48 50 ..., 20 23  7]\n",
      " [ 3 21 32 ...,  1  0 15]\n",
      " [ 8  1 10 ...,  8  0 38]]\n",
      "==== ybatch : [[14 40 36 ...,  3  5 35]\n",
      " [24 21 46 ...,  9  3  3]\n",
      " [ 2  0  1 ..., 21 52 11]\n",
      " ..., \n",
      " [48 50 10 ..., 23  7  6]\n",
      " [21 32 20 ...,  0 15  1]\n",
      " [ 1 10 19 ...,  0 38 47]]\n",
      "[1700/8500] cost: 1.4723 / Each batch learning took 0.6543 sec\n",
      "==== xbatch : [[ 8 13  4 ...,  0 22 36]\n",
      " [38 53 45 ..., 33 45 26]\n",
      " [ 6  4 17 ..., 10 50 33]\n",
      " ..., \n",
      " [ 9  4  2 ...,  0 12 14]\n",
      " [ 3  3 23 ..., 29  0 30]\n",
      " [ 2  5  9 ..., 14 16  2]]\n",
      "==== ybatch : [[13  4  8 ..., 22 36  7]\n",
      " [53 45 45 ..., 45 26  0]\n",
      " [ 4 17  0 ..., 50 33 41]\n",
      " ..., \n",
      " [ 4  2  1 ..., 12 14  3]\n",
      " [ 3 23  7 ...,  0 30  4]\n",
      " [ 5  9 12 ..., 16  2  4]]\n",
      "[1800/8500] cost: 1.4100 / Each batch learning took 0.6549 sec\n",
      "==== xbatch : [[ 1  8 21 ..., 17  4 13]\n",
      " [ 4  8 24 ..., 42  0  9]\n",
      " [10 17 11 ..., 18  1  3]\n",
      " ..., \n",
      " [30 35  7 ..., 14 12  2]\n",
      " [ 4 17  0 ..., 26  0 52]\n",
      " [ 1 15  0 ..., 12 11 19]]\n",
      "==== ybatch : [[ 8 21  8 ...,  4 13  1]\n",
      " [ 8 24  0 ...,  0  9  2]\n",
      " [17 11  3 ...,  1  3  1]\n",
      " ..., \n",
      " [35  7  6 ..., 12  2  0]\n",
      " [17  0 21 ...,  0 52  0]\n",
      " [15  0  2 ..., 11 19 16]]\n",
      "[1900/8500] cost: 1.3642 / Each batch learning took 0.6567 sec\n",
      "==== xbatch : [[27 14 17 ..., 17 28 27]\n",
      " [ 5  1  0 ..., 13  1 15]\n",
      " [67 20  0 ..., 37 74 50]\n",
      " ..., \n",
      " [15 28 31 ..., 19 14  2]\n",
      " [13 11  9 ...,  0  2 18]\n",
      " [41 33 58 ...,  1  2 34]]\n",
      "==== ybatch : [[14 17 17 ..., 28 27  9]\n",
      " [ 1  0 48 ...,  1 15  0]\n",
      " [20  0 43 ..., 74 50 33]\n",
      " ..., \n",
      " [28 31 11 ..., 14  2  1]\n",
      " [11  9 15 ...,  2 18  9]\n",
      " [33 58 50 ...,  2 34  9]]\n",
      "[2000/8500] cost: 1.4096 / Each batch learning took 0.6549 sec\n",
      "model saved to 'RNN_data/model.ckpt'\n",
      "==== xbatch : [[68 85  0 ..., 15  0 13]\n",
      " [ 0  9  3 ...,  9  2  1]\n",
      " [20 23  0 ..., 18 20 23]\n",
      " ..., \n",
      " [ 8  2  0 ...,  2 18  4]\n",
      " [ 4 13  1 ..., 35  8  3]\n",
      " [19 27 21 ...,  8  1 10]]\n",
      "==== ybatch : [[85  0 22 ...,  0 13 11]\n",
      " [ 9  3  1 ...,  2  1 26]\n",
      " [23  0 36 ..., 20 23  7]\n",
      " ..., \n",
      " [ 2  0 11 ..., 18  4  8]\n",
      " [13  1 15 ...,  8  3 10]\n",
      " [27 21 20 ...,  1 10  2]]\n",
      "[2100/8500] cost: 1.3883 / Each batch learning took 0.6557 sec\n",
      "==== xbatch : [[66 66 84 ...,  1  0 29]\n",
      " [ 0 22  0 ..., 11  8  0]\n",
      " [16  9  3 ...,  3 30  0]\n",
      " ..., \n",
      " [ 8 11  3 ..., 37 59  0]\n",
      " [36 22  0 ...,  8  2 24]\n",
      " [ 5  1  3 ..., 12  2 13]]\n",
      "==== ybatch : [[66 84 60 ...,  0 29  0]\n",
      " [22  0  9 ...,  8  0  1]\n",
      " [ 9  3  1 ..., 30  0 22]\n",
      " ..., \n",
      " [11  3 19 ..., 59  0 80]\n",
      " [22  0 76 ...,  2 24 21]\n",
      " [ 1  3  0 ...,  2 13 10]]\n",
      "[2200/8500] cost: 1.3158 / Each batch learning took 0.6551 sec\n",
      "==== xbatch : [[ 1  1 10 ...,  8  2 26]\n",
      " [ 0 36 22 ..., 25  1  2]\n",
      " [11  0  1 ...,  0 66  0]\n",
      " ..., \n",
      " [10 47 38 ..., 46 12 14]\n",
      " [40  2  0 ...,  0  9  3]\n",
      " [30  5 10 ..., 12 11  8]]\n",
      "==== ybatch : [[ 1 10  4 ...,  2 26  0]\n",
      " [36 22  0 ...,  1  2 26]\n",
      " [ 0  1 40 ..., 66  0 48]\n",
      " ..., \n",
      " [47 38 21 ..., 12 14  3]\n",
      " [ 2  0 51 ...,  9  3  3]\n",
      " [ 5 10  5 ..., 11  8 15]]\n",
      "[2300/8500] cost: 1.3296 / Each batch learning took 0.6534 sec\n",
      "==== xbatch : [[22  0 42 ...,  9 12  2]\n",
      " [ 0 39  8 ..., 18  1  9]\n",
      " [ 0  9 14 ..., 15 11  8]\n",
      " ..., \n",
      " [12 13 11 ..., 11 40 30]\n",
      " [ 8  0 28 ..., 50 45 39]\n",
      " [11 19 16 ...,  4  2  4]]\n",
      "==== ybatch : [[ 0 42 53 ..., 12  2  0]\n",
      " [39  8 12 ...,  1  9 15]\n",
      " [ 9 14 15 ..., 11  8 71]\n",
      " ..., \n",
      " [13 11  8 ..., 40 30  0]\n",
      " [ 0 28  8 ..., 45 39 37]\n",
      " [19 16 13 ...,  2  4 12]]\n",
      "[2400/8500] cost: 1.2746 / Each batch learning took 0.8295 sec\n",
      "==== xbatch : [[12 18 14 ...,  9  3  1]\n",
      " [26  0 22 ..., 26  0  5]\n",
      " [ 0 17 28 ...,  5 32  4]\n",
      " ..., \n",
      " [ 0 44  0 ..., 40  1  5]\n",
      " [68 33 56 ...,  0  1 40]\n",
      " [10 12  9 ..., 22  0  9]]\n",
      "==== ybatch : [[18 14  8 ...,  3  1  0]\n",
      " [ 0 22  8 ...,  0  5  4]\n",
      " [17 28 31 ..., 32  4  2]\n",
      " ..., \n",
      " [44  0 32 ...,  1  5  7]\n",
      " [33 56 42 ...,  1 40 16]\n",
      " [12  9 13 ...,  0  9 34]]\n",
      "[2500/8500] cost: 1.3011 / Each batch learning took 0.6607 sec\n",
      "model saved to 'RNN_data/model.ckpt'\n",
      "==== xbatch : [[ 1 40 16 ...,  9  3 25]\n",
      " [ 2 20 23 ...,  8 11 32]\n",
      " [ 2 12 18 ...,  9  2  9]\n",
      " ..., \n",
      " [11 34  1 ..., 39 38 37]\n",
      " [21 13 11 ..., 34 12  5]\n",
      " [ 9 12 18 ...,  4 12 18]]\n",
      "==== ybatch : [[40 16 67 ...,  3 25  1]\n",
      " [20 23  7 ..., 11 32 23]\n",
      " [12 18 35 ...,  2  9  0]\n",
      " ..., \n",
      " [34  1  3 ..., 38 37 10]\n",
      " [13 11  8 ..., 12  5 32]\n",
      " [12 18  4 ..., 12 18  0]]\n",
      "[2600/8500] cost: 1.2738 / Each batch learning took 0.6549 sec\n",
      "==== xbatch : [[ 6  6 16 ..., 14  8 15]\n",
      " [26  0 25 ...,  8 13 19]\n",
      " [35  0 22 ..., 12 18  9]\n",
      " ..., \n",
      " [21 24  2 ...,  0 22  0]\n",
      " [23  7  5 ...,  2  3 20]\n",
      " [14 12  2 ..., 15  0 17]]\n",
      "==== ybatch : [[ 6 16 14 ...,  8 15  1]\n",
      " [ 0 25 17 ..., 13 19  5]\n",
      " [ 0 22 36 ..., 18  9  3]\n",
      " ..., \n",
      " [24  2 18 ..., 22  0 12]\n",
      " [ 7  5  2 ...,  3 20  7]\n",
      " [12  2  0 ...,  0 17  3]]\n",
      "[2700/8500] cost: 1.2486 / Each batch learning took 0.6559 sec\n",
      "==== xbatch : [[11 12 24 ..., 14  8 24]\n",
      " [ 0 34 11 ..., 23  7  7]\n",
      " [21  4  8 ..., 53 56 39]\n",
      " ..., \n",
      " [76 38 33 ...,  5  4 25]\n",
      " [ 7 44  7 ...,  3 51 26]\n",
      " [30  5 12 ...,  3 11 27]]\n",
      "==== ybatch : [[12 24 21 ...,  8 24 23]\n",
      " [34 11  4 ...,  7  7  6]\n",
      " [ 4  8  2 ..., 56 39 37]\n",
      " ..., \n",
      " [38 33 41 ...,  4 25  8]\n",
      " [44  7  7 ..., 51 26  0]\n",
      " [ 5 12  9 ..., 11 27 14]]\n",
      "[2800/8500] cost: 1.2930 / Each batch learning took 0.6559 sec\n",
      "==== xbatch : [[36 72 54 ..., 15  1  0]\n",
      " [21 46 18 ...,  4 15  0]\n",
      " [44  7  7 ...,  2 10 13]\n",
      " ..., \n",
      " [ 0  8  5 ..., 14  8  2]\n",
      " [ 1  9 15 ...,  2  1 20]\n",
      " [12  2  7 ..., 16 28 31]]\n",
      "==== ybatch : [[72 54 54 ...,  1  0 66]\n",
      " [46 18  9 ..., 15  0 18]\n",
      " [ 7  7 36 ..., 10 13 11]\n",
      " ..., \n",
      " [ 8  5 16 ...,  8  2 26]\n",
      " [ 9 15 21 ...,  1 20 23]\n",
      " [ 2  7  0 ..., 28 31 16]]\n",
      "[2900/8500] cost: 1.1466 / Each batch learning took 0.6590 sec\n",
      "==== xbatch : [[ 1 23  0 ..., 70 35  4]\n",
      " [15 29 80 ...,  6  6  6]\n",
      " [39 56 59 ...,  0 48 53]\n",
      " ..., \n",
      " [23  7  6 ...,  1 10  1]\n",
      " [11 27 14 ..., 15 28 31]\n",
      " [ 4 17  0 ..., 23  7  6]]\n",
      "==== ybatch : [[23  0  4 ..., 35  4  8]\n",
      " [29 80 14 ...,  6  6  6]\n",
      " [56 59  7 ..., 48 53 56]\n",
      " ..., \n",
      " [ 7  6 16 ..., 10  1 19]\n",
      " [27 14  5 ..., 28 31  5]\n",
      " [17  0 21 ...,  7  6  5]]\n",
      "[3000/8500] cost: 1.2918 / Each batch learning took 0.6580 sec\n",
      "model saved to 'RNN_data/model.ckpt'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1c0c5b1200c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         train_loss, state, _ = sess.run([cost, final_state, optm]\n\u001b[0;32m---> 27\u001b[0;31m             , feed_dict={input_data: xbatch, targets: ybatch, istate: state}) \n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtotal_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mend_time\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model!\n",
    "num_epochs    = 50\n",
    "save_every    = 500\n",
    "learning_rate = 0.002\n",
    "decay_rate    = 0.97\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "summary_writer = tf.train.SummaryWriter(save_dir, graph=sess.graph)\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "init_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Learning rate scheduling \n",
    "    sess.run(tf.assign(lr, learning_rate * (decay_rate ** epoch)))\n",
    "    state     = sess.run(istate)\n",
    "    batchidx  = 0\n",
    "    for iteration in range(num_batches):\n",
    "        start_time   = time.time()\n",
    "        randbatchidx = np.random.randint(num_batches)\n",
    "        xbatch       = xbatches[batchidx]\n",
    "        ybatch       = ybatches[batchidx]\n",
    "        batchidx     = batchidx + 1\n",
    "        \n",
    "        # Note that, num_batches = len(xbatches)\n",
    "        # Train! \n",
    "        train_loss, state, _ = sess.run([cost, final_state, optm]\n",
    "            , feed_dict={input_data: xbatch, targets: ybatch, istate: state}) \n",
    "        total_iter = epoch*num_batches + iteration\n",
    "        end_time     = time.time();\n",
    "        duration     = end_time - start_time\n",
    "        \n",
    "        if total_iter % 100 == 0:\n",
    "            print(\"==== xbatch : {0}\".format(xbatch))\n",
    "            print(\"==== ybatch : {0}\".format(ybatch))\n",
    "            print (\"[%d/%d] cost: %.4f / Each batch learning took %.4f sec\" \n",
    "                   % (total_iter, num_epochs*num_batches, train_loss, duration))\n",
    "        if total_iter % save_every == 0: \n",
    "            ckpt_path = os.path.join(save_dir, 'model.ckpt')\n",
    "            saver.save(sess, ckpt_path, global_step = total_iter)\n",
    "            # Save network! \n",
    "            print(\"model saved to '%s'\" % (ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
