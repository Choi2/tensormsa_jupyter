{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Model (with onehot encoder)\n",
    "Hoyai 를 통해서 Seq2Seq 모델을 구성하는 방법을 설명하고자 한다. Hoyai 에서 제공하는 Seq2Seq 는 단어의 Embedding 방식을 Google 에서 제공하는 Default Embedding 을 사용하지 않고, 별도의 Word2Vec 를 구성하여 연동할 수 있도록 한다. 또한, 한글 POS 처리 등 부분들도 내부적으로 지원하여 한글에 대한 최적처리를 지원하고 있다. <br>\n",
    "그럼 두가지 중요한 개념인 Seq2Seq 와 Word2Vec 에 대해서 간단하게 설명하도록 하겠다. 아래의 그림은 Seq2Seq 를 간단하게 설명하는 그림이다.그렇다면 Seq2Seq 는 어떤 경우에 활용할 수 있는 모델일까?<br><br>\n",
    "<b>[번역]</b> 가장 대표적인 사용예는 번역이 될 것이다. 요즘 Google 번역기가 매우 좋아진 것도 바로 이런 이유이다. <br>\n",
    "<b>Encode : 안녕하세요. 오늘 기분은 어떠세요?</b> <br>\n",
    "<b>Decode : Hello. How are you feel today?</b> <br>\n",
    "위와 같이 데이터를 구성하고 아래의 Network 을 이용하여 훈련을 시키면 \"안녕하세요. 오늘 기분은 어떠세요?\" 라고 입력을 하였을때, \"Hello. How are you feel today?\"라는 답을 하는 네트워크가 구성되는 것이다. <br><br>\n",
    "<b>[대화]</b>번역뿐만 아니라 간단한 문/답에도 아래와 같이 적용해 볼 수 있을 것이다. (※단순 Seq2Seq 로 Alex 같은 것은 구현할 수 없다)<br>\n",
    "<b>Encode : 안녕하세요!? </b> <br>\n",
    "<b>Decode : 잘가세요!?</b> <br><br>\n",
    "<b>[분류]</b>Time Series 한 Classification 문제도 생각해 볼 수가 있다. 아래의 데이터를 주가의 흐름이라고 하자 <br>\n",
    "<b>Encode : 1900, 1800, 1700, 1600, 2000 </b> <br>\n",
    "<b>Decode : Up/Down</b> <br>\n",
    "예를 들면 종합 주가지수가 1900, 1800, 1700, 1600, 2000  와 같이 변동해 왔을때, 내일의 주가는 내려갈까요? 올라갈까요? 와 같은 형태의 질문도 훈련을 할 수가 있을 것이다. \n",
    "\n",
    "<br><br><b>[그림1] Seq2Seq <b> <br>\n",
    "<img src=\"../../images/seq2seq_desc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and Preprocessing \n",
    "데이터에 대한 전처리는 어떤 종류의 Deep Learning 을 시도하여도 매우 중요한 문제이겠지만, NLP 에서는 더욱더 Critical 하고 어렵고 복잡한 문제라고 개인적으로 생각한다. 데이터에 대한 전처리는 구문분석, Entity 분석, Word2Vec, POS Tag, Doc2Vec 등.. 많은 것들이 있을 수 있지만 우리는 Seq2Seq 를 효과적으로 구성하기 위하여 POS Tagging 과 Word2Vec 두 가지를 사용하였다. 기본적으로 문장을 신경망에 훈련할때는 아래와 같은 과정을 거치게 된다. <br><br>\n",
    "<b>(1) Tockenizer:</b> 문장을 띄어쓰기, 마침표 등 기준으로 단어 단위로 분리를 해줘야 한다. <br>\n",
    "<b>(2) POS Tag: </b>모든 언어가 그렇지만, 같은 단어가 상황에 따라 다르게 사용될 수 있기 때문에, Word/품사와 같이 분류해준다 <br>\n",
    "<b>(3) Special Char :</b> 알수 없는 문자, Decoder부 시작, Padding 부분 등을 표시하는 Vecotr를 정의하고 앞뒤에 추가해 준다. <br>\n",
    "<b>(4) 사전구성 :</b> (3)의 특수문자를 포함하여 사용할 단어와 Index 를 맵핑하는 형태로 사전을 전부 구성해야 한다. <br>\n",
    "<b>(5) Embedding :</b> 신경망은 결론적으로 문자 자체를 입력으로 받을 수는 없다. 때문에 Vector 로 바꿔줘야한다.<br>\n",
    "<b>※ 이때 사용할 수 있는 것이 OneHot Encoding 과 Word2Vecotr 이다. (두 가지의 차이가 매우 중요하다!) <b><br>\n",
    "\n",
    "<br><br><b>[그림2] OneHot Encoder </b> <br>\n",
    "<img src=\"../../images/onehot_encoder.png\"> <br>\n",
    "OneHot Encoder는 위에 그림에서 보는 것 처럼 사전에 포함된 단어의 수 만큼 Vector를 구성하고 해당 단어의 Index 에 대응되는 값만 1로 표시해주는 형태라고 보면 된다. 이런 방식의 문제는 (1) 단어의 수가 많아지면 Vector의 크기가 계속 커진다. (2) 단어간의 유사성을 표현할 수 없어 유사한 표현을 처리하기는 어렵다. 두가지 정도일탠데, (2) 번 사항은 실제로 Seq2Seq 모델을 만들었을때 굉장히 큰 성능의 차이를 보인다고 생각한다. <br>\n",
    "\n",
    "※ 다만, 지금까지의 테스트 결과 W2V Model 이 잘 훈련되어 있지 않아, 단어간의 Vector 차이가 너무 적은 경우, 또 훈련하고자 하는 대화의 양은 적은데 Word2Vector 모델에서 관리하는 단어는 너무 많은 경우 오히려 Onehot 보다 성능이 나쁜 경우도 확인하였다. 하지만, 장기적으로 더 많은 대화를 이해하고 조금 다른 문맥도 이해하게 만들고자 한다면, 더 많은 데이터를 확보하고 Word2Vec 을 활용하는 것이 좋다고 생각한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoyai 에서 실행하기\n",
    "실제로 데이터 전처리부터 Seq2Seq 모델까지 직접만드려고 하면 굉장히 많은 양의 코드를 개발하고 테스트 해야할 것이다. 하지만 Hoyai 에서는 굉장히 쉽게 API 를 통해서 모델을 만들 수 있도록 지원하고 있다. 그러면 필요한 Step 을 먼저 정리해 보자. \n",
    "\n",
    "<b>(1) Word2Vec 모델 :</b> 첫번째로 해야할 일은 Word2Vec 모델을 만드는 일이다. 상세한 방법은 <b>(상세설명 : [link to source])</b> 를 참조하면 된다.  <br> ※ Hoyai 에서는 각각의 모델이 별도로 관리되고 모델을 다른 모델에서 다시 활용 할 수 있도록 시스템이 구성되어 있다.<br>\n",
    "<b>(2) Network 등록 :</b> 다시 Seq2Seq 로 돌아와서 비지니스 측면에서 모델을 생성한다. (상세설명 : [link to source](../01.common/common_neural_network_define.ipynb))</b> <br>\n",
    "<b>(3) Network 버전 등록 :</b> 네트워크의 버전 및 Stage 를 등록한다.  <br>\n",
    "<b>(4) Graph Flow 등록 :</b> 실행 내용을 정의한다.  <br>\n",
    "<b>(5) Graph Node Parm 등록 :</b> 각 Node 에서 실행할 내용을 정의한다 <br>\n",
    "<b>(6) Run Train :</b> 정의된 Graph 내용을 실행한다. <br>\n",
    "<b>(7) Predict Service :</b> API 를 통해 예측 서비스를 제공할 수 있다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2)(3) Network 등록/버전 등록\n",
    "해당 부분은 모든 네트워크가 동일하게 적용되는 부분으로 자세한 설명은 (상세설명 : [link to source](../01.common/common_neural_network_define.ipynb))를 참조 할 수 있도록 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json, os\n",
    "\n",
    "nn_id = 'nn000999'  # put some key value you want to test\n",
    "\n",
    "url = \"{0}:{1}\".format(os.environ['HOSTNAME'] , \"8000\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : nn000999\n",
      "evaluation result : nn000999\n"
     ]
    }
   ],
   "source": [
    "####(1) 네트워크 생성 ####\n",
    "resp = requests.post('http://' + url + '/api/v1/type/common/target/nninfo/nnid/' + nn_id + '/',\n",
    "                     json={\n",
    "                         \"biz_cate\": \"MES\",\n",
    "                         \"biz_sub_cate\": \"M60\",\n",
    "                         \"nn_title\" : \"test\",\n",
    "                         \"nn_desc\": \"test desc\",\n",
    "                         \"use_flag\" : \"Y\",\n",
    "                         \"dir\": \"purpose?\",\n",
    "                         \"config\": \"N\"\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))\n",
    "\n",
    "####(2) 버전 생성 ####\n",
    "resp = requests.post('http://' + url + '/api/v1/type/common/target/nninfo/nnid/' + nn_id + '/version/',\n",
    "                 json={\n",
    "                     \"nn_def_list_info_nn_id\": \"\",\n",
    "                     \"nn_wf_ver_info\": \"test version info\",\n",
    "                     \"condition\": \"1\",\n",
    "                     \"active_flag\": \"Y\"\n",
    "                 })\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) Graph 생성\n",
    "실제로 어떤 순서로 어떻게 동작을 할 것인가를 구성하는 단계로 Graph 구성에 대한 추가적인 설명이 필요할 것이라고 생각된다. <br>\n",
    "Network - Version - Stage 형태로 데이터가 구성되며, Stage 안에는 아래의 그림과 같이 개발 기능단위 컴포넌트 (앞으로는 Node 라고 지칭)를 정의하고 Relation 을 정의할 수 있다. Graph Flow 를 정의하고, 각 Node 의 동작 내용을 상세 정의하는 과정을 수행하면, 실제로 전체 Flow 를 실행할 준비가 끝나는 것이다. \n",
    "\n",
    "\n",
    "<br><img src=\"../../images/graph_flow.jpg\"> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) Graph Flow 생성 \n",
    "실제로 수행하고자하는 Flow 를 구성하기 위한 API 를 호출한다. Dynamic 하게 Flow 를 구성할 수도 있지만, 정해진 Flow 를 강제로 생성하기 위한 API 를 사용하도록 한다. 강제로 생성되는 Flow 는 아래와 같이 총 6개의 Node 로 구성되어 있으며, 각 Node 는 정의되어야 한다.  <br> \n",
    "<b>Train Data >> Feed >> Network << Evaluation << Test Data << Feed <b>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : seq2seq_csv\n"
     ]
    }
   ],
   "source": [
    "# Work Flow 틀을 구성하도로고 지시한다. (정해진 틀을 강제로 생성)\n",
    "resp = requests.post('http://' + url + '/api/v1/type/wf/target/init/mode/simple/' + nn_id +'/wfver/1/',\n",
    "                     json={\n",
    "                         \"type\": 'seq2seq_csv'\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) 데이터 업로드 \n",
    "데이터를 Jyupter를 통해서 업로드 하는 과정은 아래와 같다. <br>\n",
    "1. <b>Jupyter Upload Button 을 활용하여 data 폴더에 데이터를 옮긴다.</b> <br>\n",
    "※ 정확한 위치는 여기 :  /home/dev/hoyai_jupyter/data/ <br>\n",
    "2. <b>Data Upload API 를 실행하여 실제 Src 폴더로 데이터를 이동시킨다.</b> <br>\n",
    "※ 만약 자신의 Local 에서 Jupyter 를 실행하였다면, 2번만 바로 수행해도 될 것이다. <br> \n",
    "3. <b>테스트에서 사용하고자 하는 데이터 </b> <br> \n",
    " (데이터 보기 : [link to source](../../data/seq2seq.csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : ['1 file upload success']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "return_dict = {}\n",
    "return_dict['test'] = open('../../data/seq2seq_mansearch.csv', 'rb')\n",
    "\n",
    "resp = requests.post('http://' + url + '/api/v1/type/wf/state/framedata/src/local/form/raw/prg/source/nnid/'+nn_id+'/ver/1/node/data_csv_node/',\n",
    "                     files = return_dict)\n",
    "\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) Node 속성 정의 - Train Data Node\n",
    "Train 을 위한 데이터를 수집하고 전처리하여 저장하는 과정을 Node 로 정의한다. DataNode는 세부적으로 3가지 Step 으로 구성되어 있다. <br>\n",
    "Frame Data Node 를 사용하는 경우 실질적인 POS Tagging 처리 등은 Feeder 에서 이루어 지게 된다. <br>\n",
    "1. <b>데이터 수집에 대한 정의</b>  <br>\n",
    "2. <b>데이터 전처리에 대한 정의</b> <br>\n",
    "3. <b>데이터 저장에 대한 정의</b> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : /hoya_str_root/nn000999/1/data_csv_node\n"
     ]
    }
   ],
   "source": [
    "# (1) Train Data Node 의 속성을 정의\n",
    "# 어디서 Source 를 어떻게 가지고 올것인지 파라메터 정의 \n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/framedata/src/local/form/raw/prg/source/nnid/'+nn_id+'/ver/1/node/data_csv_node/',\n",
    "                     json={\n",
    "                         \"type\": \"csv\",\n",
    "                         \"source_server\": \"local\",\n",
    "                         \"source_sql\": \"all\",\n",
    "                     })\n",
    "\n",
    "# 전처리는 어떤 것을 할지 정의\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/framedata/src/local/form/raw/prg/pre/nnid/'+nn_id+'/ver/1/node/data_csv_node/',\n",
    "                      json={\n",
    "                          \"preprocess\":  \"none\",\n",
    "                      })\n",
    "# 전처리가 완료된 데이터는 어디에 저장을 할지 \n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/framedata/src/local/form/raw/prg/store/nnid/'+nn_id+'/ver/1/node/data_csv_node/',)\n",
    "\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (6) Node 속성 정의 - Data Feeder\n",
    "Feeder 는 Network 와 Data Node 사이에서 Network 에서 사용하기 적합한 형태로 각종 가공 처리를 하기 위한 용도로 개발되었으며, 아래의 Node 의 경우 Frame Data 에서 Seq2Seq 모델로 데이터를 연결하기 위한 용도로 사용된다. <br>\n",
    "본 예제에서는 csv 데이터를 읽어서 Seq2seq 에 Feed 해주는 형태로 어떤 csv 컬럼을 각각 Encode 부와 Decode 부로 사용할 것인지 정의하고 pos Tagging에 어떤 알고리즘을 사용할 것인지, 그리고 문장의 길이는 최대 어느정도까지 인지할 것인지 지정하도록 한다. <br>\n",
    "- encode_column : Csv 파일에서 Encode 에 사용할 컬럼 명 <br>\n",
    "- decode_column : Csv 파일에서 Decode 에 사용할 컬럼 명 <br>\n",
    "- max_sentence_len : 문장의 길이를 최대 어디까지 인지할 것인지 지정 <br>\n",
    "- preprocess : 사용할 Pos Tagger 를 지정 (mecab, kkma, twiter 등) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : {'preprocess': 'mecab', 'encode_column': 'encode', 'decode_len': 2, 'encode_len': 10, 'decode_column': 'decode'}\n"
     ]
    }
   ],
   "source": [
    "# (2) Network 에 데이터를 Feed하는 Node 의 속성을 정의 \n",
    "resp = requests.post('http://' + url + '/api/v1/type/wf/state/pre/detail/feed/src/frame/net/seq2seq/nnid/'+nn_id+'/ver/1/node/feed_fr2seq/',\n",
    "                     json={\n",
    "                         \"encode_column\" : \"encode\",\n",
    "                         \"decode_column\" : \"decode\",\n",
    "                         \"encode_len\" : 10,\n",
    "                         \"decode_len\" : 2,\n",
    "                         \"preprocess\": \"mecab\",\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (7) Node 속성 정의 - Seq2Seq 신경망 정의\n",
    "Seq2Seq 모델은 Encoder부와 Decoder 부로 정의되어 있으며, 아래와 같은 파라메터들을 정의할 수 있다. <br>\n",
    "- encoder_len : 인코더 부분의 길이 지정 <br>\n",
    "- decoder_len : 디코더 부분의 길이 지정 <br>\n",
    "- encoder_depth : 인코더 Hidden Layer 깊이 <br>\n",
    "- decoder_depth : 디코더 Hidden Layer 깊이 <br>\n",
    "- cell_type : vanila, lstm, gru <br>\n",
    "- cell_size : cell 의 Vector size <br>\n",
    "- drop_out : Train  시 Drop Out Rate <br>\n",
    "- word_embed_type : onehot encoder 를 사용할 것인지, word2vector를 사용할 것인지 판단 <br>\n",
    "- word_embed_id : Word2Vector인 경우 사용한 기 훈련된 Network ID 를 지정해야 함  <br>\n",
    "- batch_size : 한번에 훈련할 데이터 건수  <br>\n",
    "- iter : 반복해서 훈련할 횟수  <br>\n",
    "- early_stop : 지정횟수 이전에 훈련을 종료하기 위한 적중률 기준  <br>\n",
    "- learning_rate : Weight 값 갱신시 사용한 Hyper Parameter  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : {'word_embed_type': 'onehot', 'encoder_depth': 2, 'decoder_len': 2, 'encoder_len': 10, 'batch_size': 100, 'cell_type': 'lstm', 'learning_rate': 0.001, 'model_path': '/hoya_model_root/nn000999/1/netconf_node', 'iter': 100, 'vocab_size': 100, 'cell_size': 500, 'word_embed_id': '', 'drop_out': 0.8, 'early_stop': 0.9, 'decoder_depth': 2}\n"
     ]
    }
   ],
   "source": [
    "# update source_info\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/netconf/detail/seq2seq/nnid/'+nn_id+'/ver/1/node/netconf_node/',\n",
    "                     json={\n",
    "                         \"encoder_len\" : 10,\n",
    "                         \"decoder_len\" : 2,\n",
    "                         \"encoder_depth\" : 2,\n",
    "                         \"decoder_depth\" : 2,\n",
    "                         \"cell_type\" : \"lstm\",   #vanila, lstm, gru\n",
    "                         \"cell_size\" : 500,\n",
    "                         \"drop_out\" : 0.8,\n",
    "                         \"word_embed_type\" : \"onehot\",   #w2v, onehot\n",
    "                         \"word_embed_id\" : \"\",\n",
    "                         \"vocab_size\" : 100,\n",
    "                         \"batch_size\" : 100,\n",
    "                         \"iter\" : 100,\n",
    "                         \"early_stop\" : 0.9,\n",
    "                         \"learning_rate\" : 0.001\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (8) Node 속성 정의 - Test Data Node\n",
    "Test 를 위한 데이터와 Feeder 를 정의합니다. 세부 내용은 Train 을 위한 데이터 정의와 동일합니다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : ['1 file upload success']\n",
      "evaluation result : {'preprocess': 'none', 'type': 'csv', 'source_path': '/hoya_src_root/nn000999/1/evaldata', 'source_parse_type': 'raw', 'store_path': '/hoya_str_root/nn000999/1/evaldata', 'multi_node_flag': None, 'source_type': 'local', 'max_sentence_len': 0, 'source_sql': 'all', 'source_server': 'local'}\n",
      "evaluation result : none\n",
      "evaluation result : /hoya_str_root/nn000999/1/evaldata\n",
      "evaluation result : {'status': '404', 'result': 'you cannot change critical values, create new version for diffrent model'}\n"
     ]
    }
   ],
   "source": [
    "# upload files for test\n",
    "return_dict = {}\n",
    "return_dict['test'] = open('../../data/seq2seq_mansearch.csv', 'rb')\n",
    "\n",
    "resp = requests.post('http://' + url + '/api/v1/type/wf/state/framedata/src/local/form/raw/prg/source/nnid/'+nn_id+'/ver/1/node/evaldata/',\n",
    "                     files = return_dict)\n",
    "\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))\n",
    "\n",
    "# 데이터 - 소스 정의\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/framedata/src/local/form/raw/prg/source/nnid/'+nn_id+'/ver/1/node/evaldata/',\n",
    "                     json={\n",
    "                         \"type\": \"csv\",\n",
    "                         \"source_server\": \"local\",\n",
    "                         \"source_sql\": \"all\",\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))\n",
    "\n",
    "# 데이터 - 전처리 정의\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/framedata/src/local/form/raw/prg/pre/nnid/'+nn_id+'/ver/1/node/evaldata/',\n",
    "                      json={\n",
    "                          \"preprocess\":  \"none\",\n",
    "                      })\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))\n",
    "\n",
    "# 데이터 - 저장 정의\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/framedata/src/local/form/raw/prg/store/nnid/'+nn_id+'/ver/1/node/evaldata/',)\n",
    "\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))\n",
    "\n",
    "\n",
    "# Feeder 정의\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/pre/detail/feed/src/frame/net/seq2seq/nnid/'+nn_id+'/ver/1/node/feed_fr2seq_test/',\n",
    "                     json={\n",
    "                         \"encode_column\" : \"encode\",\n",
    "                         \"decode_column\" : \"decode\",\n",
    "                         \"encode_len\" : 10,\n",
    "                         \"decode_len\" : 2,\n",
    "                         \"preprocess\": \"mecab\",\n",
    "                     })\n",
    "data = json.loads(resp.json())\n",
    "\n",
    "node_name = 'eval_node'\n",
    "resp = requests.put('http://' + url + '/api/v1/type/wf/state/eval/nnid/'+nn_id+'/ver/1/node/'+node_name+'/',\n",
    "                    json={\n",
    "                        \"type\": \"seq2seq\",\n",
    "                    })\n",
    "\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (9) Train 수행\n",
    "이제 훈련을 위한 준비는 모두 끝났다. 아래의 API 를 호출하면 지금까지 정의한 Graph 가 수행되며, 이 Graph 는 정주기 혹은 Event 에 의해서 자동적으로 수행되어 발생하는 데이터를 지속적으로 모델에 반영시킬 수 있도록 동작하게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : [None, None, None, None, None, {'answer': ['@ 1/SN', '@ 2/SN', '@ 2/SN', '@ 2/SN', '@ 2/SN', '@ 3/SN', '@ 3/SN', '@ 3/SN', '@ 4/SN', '@ 4/SN', '@ 4/SN', '@ 4/SN', '@ 5/SN', '@ 5/SN', '@ 5/SN', '@ 5/SN', '@ 6/SN', '@ 6/SN'], 'predict': ['@ 1/SN', '@ 2/SN', '@ 2/SN', '@ 2/SN', '@ 2/SN', '@ 3/SN', '@ 3/SN', '@ 3/SN', '@ 4/SN', '@ 4/SN', '@ 4/SN', '@ 4/SN', '@ 5/SN', '@ 5/SN', '@ 5/SN', '@ 5/SN', '@ 6/SN', '@ 6/SN'], 'accuracy': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'question': ['이미지/NNG 로/JKB 사람/NNG 을/JKO 검색/NNG 하/XSV 고/EC 싶/VX 어요/EF SF', 'MES/SL 운영/NNG 팀/NNG 에/JKB P/SL 1/SN 직급/NNG 의/JKG 남자/NNG 직원/NNG', 'ERP/SL 운영/NNG 팀/NNG 에/JKB P/SL 2/SN 직급/NNG 의/JKG 여자/NNG 직원/NNG', 'SCM/SL 운영/NNG 팀/NNG 에/JKB P/SL 4/SN 직급/NNG 의/JKG 남자/NNG 직원/NNG', '연구소/NNG 에/JKB P/SL 1/SN 직급/NNG 의/JKG 여자/NNG 직원/NNG 검색/NNG SF', '# # 김영식/NNP 팀장/NNG 소속/NNG 팀/NNG 이름/NNG 은/JX ?/SF SF', '# # 김승우/NNP 과장/NNG 소속/NNG 팀/NNG 이름/NNG 은/JX ?/SF SF', '# # 김수상/NNP 부장/NNG 소속/NNG 팀/NNG 이름/NNG 은/JX ?/SF SF', '# AI/SL 프로젝트/NNG 수행/NNG 하/XSV 는/ETM 사람/NNG 들/XSN ?/SF SF', '# 출하/NNG 운영/NNG 업무/NNG 수행/NNG 하/XSV 는/ETM 사람/NNG ?/SF SF', '# 야드/NNG 운영/NNG 업무/NNG 수행/NNG 하/XSV 는/ETM 사람/NNG ?/SF SF', '# 모바일/NNP 개발/NNG 업무/NNG 수행/NNG 하/XSV 는/ETM 사람/NNG ?/SF SF', '# 김영식/NNP 팀장/NNG 소속/NNG P/SL 2/SN 남자/NNG 직원/NNG ?/SF SF', '# 김영식/NNP 팀장/NNG 소속/NNG P/SL 3/SN 여자/NNG 직원/NNG ?/SF SF', '# 김영식/NNP 팀장/NNG 소속/NNG P/SL 4/SN 여자/NNG 직원/NNG ?/SF SF', '# 김영식/NNP 팀장/NNG 소속/NNG P/SL 1/SN 남자/NNG 직원/NNG ?/SF SF', 'P/SL 1/SN 직급/NNG 이상/NNG 김영식/NNP 팀장/NNG 소속/NNG 모바일/NNP 기술/NNG 보유/NNG', 'P/SL 2/SN 직급/NNG 이상/NNG 김영식/NNP 팀장/NNG 소속/NNG BigData/SL 기술/NNG 보유/NNG']}]\n"
     ]
    }
   ],
   "source": [
    "# Run All Workflow\n",
    "resp = requests.post('http://' + url + '/api/v1/type/runmanager/state/train/nnid/'+nn_id+'/ver/1/')\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (10) Predict 수행\n",
    "아까 훈련했던 데이터로 다시 테스트를 해보자... <br>\n",
    "테스트 결과는 별다른 필터링 없이 Start, Unknown, Pad 등 도 모두 출력 되도록 일부러 필터링 로직을 바이패스 하였다. <br>\n",
    "<table>\n",
    "<tr><td>encode</td><td>decode</td></tr>\n",
    "<tr><td>이미지로 사람을 검색하고 싶어요 </td><td>1</td></tr>\n",
    "<tr><td>MES운영팀에 P1직급의 남자 직원 검색</td><td>2 </td></tr>\n",
    "<tr><td>김영식 팀장 소속 팀 이름은 ?.</td><td>3</td></tr>\n",
    "<tr><td>AI 프로젝트 수행하는 사람들 ?</td><td>4</td></tr></tr>\n",
    "<tr><td>김영식 팀장 소속 P2남자 직원 ?</td><td>5</td></tr>\n",
    "<tr><td>P1직급 이상 김영식 팀장 소속 모바일 기술 보유 ?</td><td>6</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result :  1\n",
      "evaluation result :  2\n",
      "evaluation result :  3\n",
      "evaluation result :  4\n",
      "evaluation result :  5\n",
      "evaluation result :  6\n"
     ]
    }
   ],
   "source": [
    "resp = requests.post('http://' + url + '/api/v1/type/service/state/predict/type/seq2seq/nnid/'+nn_id+'/ver/active/',\n",
    "                     json={\"input_data\" : \"이미지로 사람을 검색하고 싶어요\" }\n",
    "                     )\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))\n",
    "\n",
    "resp = requests.post('http://' + url + '/api/v1/type/service/state/predict/type/seq2seq/nnid/'+nn_id+'/ver/active/',\n",
    "                     json={\"input_data\" : \"MES운영팀에 P1직급의 남자 직원 검색\" }\n",
    "                     )\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))\n",
    "\n",
    "resp = requests.post('http://' + url + '/api/v1/type/service/state/predict/type/seq2seq/nnid/'+nn_id+'/ver/active/',\n",
    "                     json={\"input_data\" : \"김영식 팀장 소속 팀 이름은 ?\" }\n",
    "                     )\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))\n",
    "\n",
    "resp = requests.post('http://' + url + '/api/v1/type/service/state/predict/type/seq2seq/nnid/'+nn_id+'/ver/active/',\n",
    "                     json={\"input_data\" : \"AI 프로젝트 수행하는 사람들 ?\" }\n",
    "                     )\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))\n",
    "\n",
    "resp = requests.post('http://' + url + '/api/v1/type/service/state/predict/type/seq2seq/nnid/'+nn_id+'/ver/active/',\n",
    "                     json={\"input_data\" : \"김영식 팀장 소속 P2남자 직원 ?\" }\n",
    "                     )\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))\n",
    "\n",
    "resp = requests.post('http://' + url + '/api/v1/type/service/state/predict/type/seq2seq/nnid/'+nn_id+'/ver/active/',\n",
    "                     json={\"input_data\" : \"P1직급 이상 김영식 팀장 소속 모바일 기술 보유 ?\" }\n",
    "                     )\n",
    "data = json.loads(resp.json())\n",
    "print(\"evaluation result : {0}\".format(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
