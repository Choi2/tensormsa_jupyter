{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '만나', '서', '반가워', '넌', '누구', '니', '나', '는', 'AI', '봇', '이', '란다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "qna_data = [\n",
    "                ['안녕', '만나서 반가워']\n",
    "                ,['넌누구니', '나는 AI 봇이란다.']\n",
    "#                 ,['피자 주문 할께', '페파로니 주문해줘']\n",
    "#                ,['음료는 멀로', '콜라로 해줘']\n",
    "            ]\n",
    "mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "\n",
    "train_data = list(map(lambda x : mecab.morphs(' '.join(x)) , qna_data))\n",
    "\n",
    "import itertools\n",
    "train_data = list(itertools.chain.from_iterable(train_data))\n",
    "print(list(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "bucket = np.zeros(len(train_data), dtype=np.float)\n",
    "\n",
    "for word in train_data :\n",
    "    bucket_temp = bucket.copy()\n",
    "    np.put(bucket_temp, train_data.index(word), 1)\n",
    "    print(bucket_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Vector (By Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['안녕', '만나', '서', '반가워', '넌', '누구', '니', '나', '는', 'AI', '봇', '이', '란다', '.']]\n",
      "model check : Word2Vec(vocab=14, size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "train_data = [train_data]\n",
    "print(train_data)\n",
    "\n",
    "model = word2vec.Word2Vec(size=50, window=2, min_count=1)\n",
    "model.build_vocab(train_data)\n",
    "model.train(train_data)\n",
    "print(\"model check : {0}\".format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load check : Word2Vec(vocab=14, size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./model/w2v.bin\")\n",
    "model = word2vec.Word2Vec.load(\"./model/w2v.bin\")\n",
    "print(\"model load check : {0}\".format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['니', '반가워', '서', '안녕', '나', '봇', '는', '넌', '만나', '.', 'AI', '란다', '누구', '이']\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00526203  0.00806998  0.00641448 -0.00928741 -0.00855247  0.00492507\n",
      " -0.00859462 -0.00037459  0.00933697  0.00716154 -0.00226713  0.00577714\n",
      " -0.00329574  0.00828057  0.00307698  0.00418518 -0.00317836 -0.00101071\n",
      " -0.00386956 -0.00254121  0.0071705   0.00319552  0.00326385 -0.00578259\n",
      "  0.00496802  0.00058807 -0.00067259  0.00604457 -0.00596926  0.00534698\n",
      " -0.00685895 -0.00403529 -0.0003595  -0.00023321 -0.00221119  0.00581193\n",
      " -0.00521137  0.00967642  0.00586663 -0.00576562 -0.00960775  0.00611615\n",
      " -0.00551613  0.00968471 -0.00190675 -0.0046259   0.00482867 -0.0004681\n",
      "  0.00250024  0.00866917]\n"
     ]
    }
   ],
   "source": [
    "print(model['안녕'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00438941 -0.00715526 -0.00583306  0.00407398  0.00210139 -0.00214126\n",
      "  0.00438984  0.00700825  0.00527689  0.00105609  0.0074855  -0.00216613\n",
      " -0.00099419 -0.00515468 -0.00518973 -0.0003004   0.00222211  0.00799839\n",
      " -0.00928703 -0.00358757  0.00876947  0.00976897  0.00413877  0.00184954\n",
      " -0.0007063   0.00578237 -0.00201256  0.00964103  0.00515814  0.00872782\n",
      "  0.00305379  0.00733969 -0.00929801 -0.00426201 -0.00361304 -0.00214128\n",
      " -0.00733967  0.00361662  0.00065663 -0.00482805 -0.00148609  0.00028074\n",
      " -0.00482856  0.00806901  0.00454324 -0.00350351 -0.00691609  0.00722048\n",
      "  0.00565263  0.0017324 ]\n"
     ]
    }
   ],
   "source": [
    "print(model['AI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AI', 0.24458643794059753), ('란다', 0.19687707722187042), ('니', 0.16758158802986145), ('넌', 0.08362051844596863), ('안녕', 0.05428691953420639), ('만나', -0.005056455731391907), ('이', -0.02267487905919552), ('는', -0.04669147729873657), ('.', -0.2193285971879959), ('반가워', -0.23325154185295105)]\n"
     ]
    }
   ],
   "source": [
    "result1 = model.most_similar(positive='나', negative='', topn=10)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
