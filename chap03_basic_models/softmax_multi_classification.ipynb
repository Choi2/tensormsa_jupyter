{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Softmax Multinomial Classification\n",
    "아래의 예는 Tensorflow 공홈 튜토리얼에 있는 예제와 비슷할 것이라고 예상한다. SOFTMAX 는 전체의 합이 1이 되도록 만드는 (값/전체) 형태의 단순한 함수로 값을 확률 형태로 변환하여 주는 역할을 한다. 앞의 Logistic Regression 예제와 다른 것은 데이터가 MNIST 라는 것과 softmax 를 사용하였다는 것 두가지 정도라고 생각하면 될 듯 하다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H3>(1) 데이터 준비</H3>   \n",
    "이번 예제에서 사용하는 데이터는 MNIST 라는 손글씨 이미지 데이터로 0 ~ 9 까지의 이미지를 Labling 을 하여 가지고 있다. 데이터를 읽고 전처리 하는 작업은 Tensorflow 에서 제공하는 함수를 사용함으로써 생략한다. 이 과정을 통해서 이미지 데이터는 Matrix 로 변환되며, 실행하면 나오는 결과 처럼 각 Cell 의 값은 0 ~ 1 사이가 되도록 조정한다. 친절하게도 Train Set 과 Test Set 도 알아서 분류를 해주기 때문에 그냥 Feeding 만 해주면 끝이다. 물론 나중에 CNN 등을 사용하려고 하면 Shape 을 변경하는 작업을 한번 해줘야 하지만 이건 그때 설명 하도록 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "====X====\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.38039219  0.37647063\n",
      "  0.3019608   0.46274513  0.2392157   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.35294119  0.5411765\n",
      "  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869\n",
      "  0.98431379  0.98431379  0.97254908  0.99607849  0.96078438  0.92156869\n",
      "  0.74509805  0.08235294  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.54901963  0.98431379  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.74117649  0.09019608\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.88627458  0.99607849  0.81568635\n",
      "  0.78039223  0.78039223  0.78039223  0.78039223  0.54509807  0.2392157\n",
      "  0.2392157   0.2392157   0.2392157   0.2392157   0.50196081  0.8705883\n",
      "  0.99607849  0.99607849  0.74117649  0.08235294  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14901961  0.32156864  0.0509804   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.13333334  0.83529419  0.99607849  0.99607849  0.45098042  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.41568631  0.6156863   0.99607849  0.99607849  0.95294124  0.20000002\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.09803922  0.45882356  0.89411771\n",
      "  0.89411771  0.89411771  0.99215692  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.94117653  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.26666668  0.4666667   0.86274517\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.55686277  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.14509805  0.73333335  0.99215692\n",
      "  0.99607849  0.99607849  0.99607849  0.87450987  0.80784321  0.80784321\n",
      "  0.29411766  0.26666668  0.84313732  0.99607849  0.99607849  0.45882356\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.44313729\n",
      "  0.8588236   0.99607849  0.94901967  0.89019614  0.45098042  0.34901962\n",
      "  0.12156864  0.          0.          0.          0.          0.7843138\n",
      "  0.99607849  0.9450981   0.16078432  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.66274512  0.99607849  0.6901961   0.24313727  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.18823531\n",
      "  0.90588242  0.99607849  0.91764712  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.07058824  0.48627454  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.32941177  0.99607849  0.99607849  0.65098041  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.54509807  0.99607849  0.9333334   0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.82352948  0.98039222  0.99607849  0.65882355  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.94901967  0.99607849  0.93725497  0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.34901962  0.98431379  0.9450981   0.33725491  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01960784  0.80784321  0.96470594  0.6156863   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.01568628  0.45882356  0.27058825  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.        ]\n",
      "====Y====\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "logs_path = '/tmp/tensorflow_logs/chap2-softmax'\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX = mnist.train.images\n",
    "trY = mnist.train.labels\n",
    "teX = mnist.test.images\n",
    "teY = mnist.test.labels\n",
    "\n",
    "print(\"====X====\")\n",
    "print(trX[0])\n",
    "print(\"====Y====\")\n",
    "print(trY[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>(2) Graph 구성</H3>   \n",
    "softmax_cross_entropy_with_logits 빼고는 다 유사한데 이 녀석은 결국 아래와 같이 실행하는 것과 유사한 결과를 보여준다.   \n",
    "    pred=tf.nn.softmax(tf.add(tf.matmul(x,W),b))   \n",
    "    cost =tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),reduction_indices=1))   \n",
    "Logistic 함수 대신에 Softmax 를 태우고 Cost 함수는 동일하게 Cross Entropy 를 사용하는 것으로 이해하면 된다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 784]) \n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "w = tf.Variable(tf.random_normal([784, 10], stddev=0.01))\n",
    "\n",
    "with tf.name_scope('PredFunc'):\n",
    "    py_x = tf.matmul(X, w)\n",
    "\n",
    "with tf.name_scope('CostFunc'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y)) \n",
    "    #pred=tf.nn.softmax(tf.add(tf.matmul(x,W),b))\n",
    "    #cost =tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),reduction_indices=1))\n",
    "\n",
    "with tf.name_scope('Optimize'):\n",
    "    # Train : Gradient Descent 를 사용하여 훈련 \n",
    "    train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) \n",
    "\n",
    "with tf.name_scope('Predict'):\n",
    "    # Predict : Feed forwarding 을 통해 예측 \n",
    "    predict_op = tf.argmax(py_x, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H3>(3) Session 실행</H3>   \n",
    "훈련에 사용할 데이터는 아래와 같이 구성되어 있다.     \n",
    "  [X 데이터]    [Y 데이터]  \n",
    "[ 1.  2.  1.] [ 0.  0.  1.]  \n",
    "위의 예를 보면 1,2,1 은 Y 데이터를 순서대로 A,B,C 라고 했을 때 [0, 0, 1] 임으로 C 라고 보면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 0 ,Cost : 2.2698543071746826, Acc : 0.157\n",
      "Count : 100 ,Cost : 0.7968802452087402, Acc : 0.8527\n",
      "Count : 200 ,Cost : 0.6116818189620972, Acc : 0.8709\n",
      "Count : 300 ,Cost : 0.5362462401390076, Acc : 0.8794\n",
      "Count : 400 ,Cost : 0.49357014894485474, Acc : 0.8858\n",
      "Count : 500 ,Cost : 0.4654320180416107, Acc : 0.8888\n",
      "Count : 600 ,Cost : 0.44515296816825867, Acc : 0.8922\n",
      "Count : 700 ,Cost : 0.42966121435165405, Acc : 0.8952\n",
      "Count : 800 ,Cost : 0.4173334538936615, Acc : 0.8978\n",
      "Count : 900 ,Cost : 0.4072195887565613, Acc : 0.8993\n",
      "weight vector : [[ 0.00052955  0.01681258  0.00108687 ..., -0.00922938 -0.01546925\n",
      "   0.00862249]\n",
      " [ 0.0032162   0.01096945  0.0026695  ...,  0.01006753  0.01104653\n",
      "  -0.00762846]\n",
      " [ 0.00403349  0.0114881  -0.00125678 ..., -0.00512651  0.02450618\n",
      "   0.00307375]\n",
      " ..., \n",
      " [-0.0005587  -0.0076014   0.00812096 ...,  0.00251655  0.01240145\n",
      "   0.01080104]\n",
      " [-0.00866541 -0.00719614  0.00042496 ..., -0.0199285   0.01253135\n",
      "   0.01911571]\n",
      " [ 0.00811898  0.03573269 -0.00963379 ..., -0.01513515 -0.00378702\n",
      "  -0.02004389]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    for i in range(1000):\n",
    "        # 128개씩 Train 수행 \n",
    "        #for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n",
    "            #sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "        \n",
    "        # 한번에 Train 수행 \n",
    "        sess.run(train_op, feed_dict={X: trX, Y: trY})\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            feed = {X: trX, Y: trY}\n",
    "            # Cost 값 출력 \n",
    "            print('Count : {0} ,Cost : {1}, Acc : {2}'.format(i, sess.run(cost, feed_dict=feed), \n",
    "                                                     np.mean(np.argmax(teY, axis=1) == sess.run(predict_op, feed_dict={X: teX}))))\n",
    "\n",
    "    \n",
    "    print('weight vector : {0}'.format(sess.run(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
